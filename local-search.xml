<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Supervised Contrastive Learning</title>
    <link href="/2024/06/20/Supervised/"/>
    <url>/2024/06/20/Supervised/</url>
    
    <content type="html"><![CDATA[<p>This paper can be considered an improvement over self-supervisedcontrastive learning, offering a method that helps enhance the qualityof features.</p><h3id="introduction-to-self-supervised-contrastive-learning">Introductionto Self-Supervised Contrastive Learning</h3><img src="/2024/06/20/Supervised/1.png" class=""><p>Self-supervised learning involves pre-training a model using a largeamount of unlabeled data to learn data representations. These learnedrepresentations are then used in various downstream tasks, where theyare connected to different classifiers/regressors and trained usinglabeled data from the downstream tasks, making them suitable for thosetasks.</p><p>Essentially, this maps the data onto a high-dimensional hypersphere,where vectors with similar features have high cosine similarity andvectors with dissimilar features are far apart. Word embedding seems toutilize this concept.</p><p>Different models define their losses slightly differently. Here arethe two most classic models:</p><h5 id="simclr">SimCLR</h5><p><span class="math display">\[ L = -\log \frac{\exp(\text{sim}(h_i,h_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \ne i]} \exp(\text{sim}(h_i,h_k)/\tau)} \]</span></p><h5 id="moco">MoCo</h5><p>Momentum Contrast (MoCo) uses a dynamically updated queue to storenegative samples. Its loss function is similar to SimCLR but employs amomentum encoder to generate contrastive representations. <spanclass="math display">\[ L = -\log \frac{\exp(\text{sim}(h_q,h_k^+)/\tau)}{\sum_{i=0}^{K} \exp(\text{sim}(h_q, h_{k_i})/\tau)}\]</span> Where: - $ h_q $ is the representation of the query sample. -$ h_k^+ $ is the representation of the positive sample. - $ h_{k_i} $ isthe representation of the negative samples in the queue.</p><p>While this method can learn good features, it has a shortcoming: itdoes not consider the correlation of features among different imagesbelonging to the same class. <img src="/2024/06/20/Supervised/2.png" class=""></p><h3 id="supervised-contrastive-learning">Supervised ContrastiveLearning</h3><p>To make the features of similar images close to each other, classinformation is used to determine which images belong to the same class.Therefore, the method's name changes from "self-supervised" to"supervised". The basis for contrastive learning shifts from "whetherthey come from the same image" to "whether they belong to the sameclass". The loss function used in training becomes:</p><img src="/2024/06/20/Supervised/3.png" class=""><p>The authors designed two types of loss functions, one with theweighting sum outside the log and one with it inside. Experiments foundthat the first type works better.</p>]]></content>
    
    
    <categories>
      
      <category>MATH</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SCL, SSCL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kullback-Leibler Divergence</title>
    <link href="/2024/06/20/Divergence/"/>
    <url>/2024/06/20/Divergence/</url>
    
    <content type="html"><![CDATA[<p>Before introducing KL divergence, we need to understand CE lossfirst.</p><h3 id="cross-entropy">Cross-entropy</h3><p>Why not MSE: In statistical learning, when we need to predict aspecific value, such as in a regression model, MSE can be used to definethe loss. However, if the final prediction is to select one frommultiple classes, i.e., the prediction results will be probabilitiesdistributed across different classes, we need to use cross-entropyloss.</p><p>For the general definition of entropy, we usually consider:</p><p><span class="math display">\[H=-\sum P(X)\log P(X)\]</span></p><p>Where <span class="math inline">\(P(x)\)</span> is the probability(frequency) of the event occurring, <span class="math inline">\(-\logP(x_i) = \log \frac{1}{P(x_i)}\)</span>, can be understood as the numberof occurrences of the event within a unit statistical count. Thisformula well demonstrates the nature of entropy: when an event is verycertain (probability close to 0 or 1), its entropy is close to 0;conversely, if we are unsure, such as flipping a fair coin, the entropyis larger.</p><p>As for cross-entropy, as the name suggests, when the true probabilitydistribution of an event is <span class="math inline">\(P(X)\)</span>,and we use an approximate distribution (Q(X)) to fit it, the entropy atthis time should be:</p><p><span class="math display">\[H(P, Q)=-\sum P(X)\log Q(X)\]</span></p><p>In the cross-entropy loss function, <spanclass="math inline">\(P(X)\)</span> generally contains the one-hotencoding of the true labels. For example, for binary classificationproblems, the well-known BCELoss() should be:</p><p><spanclass="math display">\[BCE=-y\log(\hat{y})-(1-y)\log(1-\hat{y})\]</span></p><h3 id="kl-divergence">KL Divergence</h3><p>KL divergence is used to measure the difference between probabilitydistributions. More specifically, it describes the amount of informationloss when we use <span class="math inline">\(Q(X)\)</span> to fit <spanclass="math inline">\(P(X)\)</span>:</p><p><span class="math display">\[D_{KL}(P||Q)=H(P,Q)-H(P)=\sumP(X)\log\frac{P(X)}{Q(X)}\]</span></p><p>Clearly, KL divergence is not symmetric, and therefore it is not ametric.</p><p>Noting that if <span class="math inline">\(g\)</span> is anyreal-valued measurable function, and <spanclass="math inline">\(\varphi\)</span> is a convex function within therange of <span class="math inline">\(g\)</span>, then</p><figure><imgsrc="https://wikimedia.org/api/rest_v1/media/math/render/svg/d33a0b42492bc7a9c197ec7c8717f5a073236f5e"alt="{({-}^{}g(x)f(x),dx){-}^{}(g(x))f(x),dx.}" /><figcaptionaria-hidden="true">{(<em>{-}^{}g(x)f(x),dx)</em>{-}^{}(g(x))f(x),dx.}</figcaption></figure><p>Using this property, it is not difficult to find:</p><p><span class="math display">\[D_{KL}(P||Q)=\intP(X)\log\frac{P(X)}{Q(X)} \ge -\log\int Q(X) =0 \]</span></p><p>Example: Calculating KL divergence between two multivariate Gaussiandistributions: <img src="/2024/06/20/Divergence/1.jpg" class=""></p>]]></content>
    
    
    <categories>
      
      <category>MATH</category>
      
    </categories>
    
    
    <tags>
      
      <tag>KL, CE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/08/hello-world/"/>
    <url>/2024/06/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>

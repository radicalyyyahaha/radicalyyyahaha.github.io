<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Implementation</title>
    <link href="/2024/10/21/Implementation/"/>
    <url>/2024/10/21/Implementation/</url>
    
    <content type="html"><![CDATA[<h1 id="overview">Overview</h1><p>This post</p><h3id="from-quantity-to-quality-boosting-llm-performance-with-self-guided-data-selection-for-instruction-tuning">FromQuantity to Quality: Boosting LLM Performance with Self-Guided DataSelection for Instruction Tuning</h3><p>One sentence summary: Let the model choose the data for itself #####Design experiment 1. Use k-means to filter out 1k instructions tofine-tune the data for training and train only one epoch 2. Thischerry-model is then used to filter the data in reverse, using the IFDmetric 3. <span class="math display">\[IFD_\theta(Q,A)=\frac{s_\theta(A|Q)}{s_\theta(A)}\]</span></p><img src="/2024/10/21/Implementation/1.png" class=""><ol start="4" type="1"><li>This formula selects data with IFD less than 1 and a high ratio,which means that these data are difficult for the model to learn</li><li>Then use this part to filter out more valuable training data totrain the model</li></ol><h5 id="reproduce-process">Reproduce process</h5><p>Use configuration: <img src="/2024/10/21/Implementation/2.png" class=""> 1. Select 1k pieces of data<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_pre.pt \<br>    --model_name_or_path &quot;meta-llama/Llama-3.1-8B&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod pre<br></code></pre></td></tr></table></figure></p><p>--data_path: The targeted dataset in the Alpaca format</p><p>--save_path: The path to save the .pt file containing embeddings orscores</p><p>--prompt: The prompt type used for training and selecting data, canchoose between alpaca or wiz</p><p>Change in <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">        <span class="hljs-keyword">elif</span> args.prompt == <span class="hljs-string">&#x27;code&#x27;</span>:<br>            input_i = data_i[<span class="hljs-string">&#x27;prompt&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;prompt&#x27;</span> <span class="hljs-keyword">in</span> data_i.keys() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span><br>            <span class="hljs-keyword">if</span> input_i == <span class="hljs-string">&#x27;&#x27;</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_no_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>            <span class="hljs-keyword">else</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i,<span class="hljs-string">&#x27;prompt&#x27;</span>:input_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>                <br>           <br><span class="hljs-keyword">and</span> some other replacement of <span class="hljs-built_in">input</span>-prompt, output-response<br></code></pre></td></tr></table></figure></p><p>--mod: pre used for getting needed embeddings or scores on selectingpre-experienced samples and cherry used for cherry</p><img src="/2024/10/21/Implementation/3.png" class=""><p>Some parameters are on the meta device because they were offloaded tothe cpu. Reason: Probably because of insufficient GPU memory</p><p>It was estimated to be about 25min, but it was actually very, verylong... But The author also admitted it in limitation. the mainlimitation of this method is the inconvenience of training thepre-experienced model.</p><p>So I randomly selected 1k pieces of data to feel ~ Time Used:27.98344091176987 (min)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_cluster.py \<br>    --pt_data_path code_data_pre.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_pre.json \<br>    --sample_num 8 \<br>    --kmeans_num_clusters 100 \<br>    --low_th 25 \<br>    --up_th 75<br></code></pre></td></tr></table></figure><p>--pt_data_path: The .pt file from previous step containing neededembeddings or scores</p><p>--json_data_path: The targeted dataset in the Alpaca format</p><p>--json_save_path: The path to save the selected pre-experiencedsamples</p><p>--sample_num: How many samples will be selected in each cluster</p><p>--kmeans_num_clusters: How many clusters will be generated byK-Means</p><p>--low_th and --up_th: The lower and Upper threshold for selectingsamples within each cluster</p><p>In an instant, 420 items were selected (why 420 items? Is it becauseeach cluster is not selected?)</p><ol start="2" type="1"><li>Train Pre-Experienced Model <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">!pip install unsloth<br></code></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> FastLanguageModel<br><span class="hljs-keyword">import</span> torch<br>max_seq_length = <span class="hljs-number">2048</span> <span class="hljs-comment"># Choose any! We auto support RoPE Scaling internally!</span><br>dtype = <span class="hljs-literal">None</span> <span class="hljs-comment"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span><br>load_in_4bit = <span class="hljs-literal">True</span> <span class="hljs-comment"># Use 4bit quantization to reduce memory usage. Can be False.</span><br><br><br>model, tokenizer = FastLanguageModel.from_pretrained(<br>    model_name = <span class="hljs-string">&quot;unsloth/Meta-Llama-3.1-8B&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dtype = dtype,<br>    load_in_4bit = load_in_4bit,<br>    <span class="hljs-comment"># token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span><br>)<br><br><br>model = FastLanguageModel.get_peft_model(<br>    model,<br>    r = <span class="hljs-number">16</span>, <span class="hljs-comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span><br>    target_modules = [<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>,<br>                      <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>,],<br>    lora_alpha = <span class="hljs-number">16</span>,<br>    lora_dropout = <span class="hljs-number">0</span>, <span class="hljs-comment"># Supports any, but = 0 is optimized</span><br>    bias = <span class="hljs-string">&quot;none&quot;</span>,    <span class="hljs-comment"># Supports any, but = &quot;none&quot; is optimized</span><br>    <span class="hljs-comment"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span><br>    use_gradient_checkpointing = <span class="hljs-string">&quot;unsloth&quot;</span>, <span class="hljs-comment"># True or &quot;unsloth&quot; for very long context</span><br>    random_state = <span class="hljs-number">3407</span>,<br>    use_rslora = <span class="hljs-literal">False</span>,  <span class="hljs-comment"># We support rank stabilized LoRA</span><br>    loftq_config = <span class="hljs-literal">None</span>, <span class="hljs-comment"># And LoftQ</span><br>)<br><br><br>alpaca_prompt = <span class="hljs-string">&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Instruction:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Input:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Response:</span><br><span class="hljs-string">&#123;&#125;&quot;&quot;&quot;</span><br><br>EOS_TOKEN = tokenizer.eos_token <span class="hljs-comment"># Must add EOS_TOKEN</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">formatting_prompts_func</span>(<span class="hljs-params">examples</span>):<br>    instructions = examples[<span class="hljs-string">&quot;instruction&quot;</span>]<br>    inputs       = examples[<span class="hljs-string">&quot;prompt&quot;</span>]<br>    outputs      = examples[<span class="hljs-string">&quot;response&quot;</span>]<br>    texts = []<br>    <span class="hljs-keyword">for</span> instruction, <span class="hljs-built_in">input</span>, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(instructions, inputs, outputs):<br>        <span class="hljs-comment"># Must add EOS_TOKEN, otherwise your generation will go on forever!</span><br>        text = alpaca_prompt.<span class="hljs-built_in">format</span>(instruction, <span class="hljs-built_in">input</span>, output) + EOS_TOKEN<br>        texts.append(text)<br>    <span class="hljs-keyword">return</span> &#123; <span class="hljs-string">&quot;text&quot;</span> : texts, &#125;<br><span class="hljs-keyword">pass</span><br><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;code_data_pre.json&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = dataset.<span class="hljs-built_in">map</span>(formatting_prompts_func, batched = <span class="hljs-literal">True</span>,)<br><br><br><br><span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTTrainer<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> is_bfloat16_supported<br><br>trainer = SFTTrainer(<br>    model = model,<br>    tokenizer = tokenizer,<br>    train_dataset = dataset,<br>    dataset_text_field = <span class="hljs-string">&quot;text&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dataset_num_proc = <span class="hljs-number">2</span>,<br>    packing = <span class="hljs-literal">False</span>, <span class="hljs-comment"># Can make training 5x faster for short sequences.</span><br>    args = TrainingArguments(<br>        per_device_train_batch_size = <span class="hljs-number">2</span>,<br>        gradient_accumulation_steps = <span class="hljs-number">4</span>,<br>        warmup_steps = <span class="hljs-number">5</span>,<br>        num_train_epochs = <span class="hljs-number">1</span>, <span class="hljs-comment"># Set this for 1 full training run.</span><br>        <span class="hljs-comment">#max_steps = 60,</span><br>        learning_rate = <span class="hljs-number">2e-4</span>,<br>        fp16 = <span class="hljs-keyword">not</span> is_bfloat16_supported(),<br>        bf16 = is_bfloat16_supported(),<br>        logging_steps = <span class="hljs-number">1</span>,<br>        optim = <span class="hljs-string">&quot;adamw_8bit&quot;</span>,<br>        weight_decay = <span class="hljs-number">0.01</span>,<br>        lr_scheduler_type = <span class="hljs-string">&quot;linear&quot;</span>,<br>        seed = <span class="hljs-number">3407</span>,<br>        output_dir = <span class="hljs-string">&quot;outputs&quot;</span>,<br>    ),<br>)<br><br>trainer.train()<br></code></pre></td></tr></table></figure><p>Time Used: 8:03min</p><ol start="3" type="1"><li>通过IFD选cherry data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_cherry.pt \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod cherry<br></code></pre></td></tr></table></figure> Time Used: 6.185897000630697(min)</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_IFD.py \<br>    --pt_data_path code_data_cherry.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_cherry.json \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 2048 \<br>    --sample_rate 0.06 \<br>    --prompt code<br></code></pre></td></tr></table></figure><p>--sample_rate: How many cherry samples you would like to select? Youcan also use --sample_number to set the exact number of samples.Finally, 60 pieces of data with IFD&lt;1 were obtained, and soted sortedthem from smallest to largest</p><p>Smallest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` Your code should pass the following test case: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) assert elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Construct a Python function `create_folds(data, count)` to create a list of folds from the given data, where each fold is a subsequence of the original data. The length of each fold should be approximately equal with at most a difference of 1. The function should satisfy the following assertion: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ### Response [Reasoning] To create a Python function that generates a list of folds from given data with each fold having approximately equal length (with at most a difference of 1 between any two folds), you can follow these steps: 1. Return an empty list immediately if `count` is non-positive since it&#x27;s not possible to create a valid number of folds. 2. Divide the total length of the data by `count`, rounding down, to get the minimum number of items each fold should have. 3. Calculate the remainder to see how many extra items need to be distributed among the folds. They are distributed one by one to the first `remainder` folds. 4. Use a loop to create each fold, adjusting the starting and ending indices based on the calculations above to ensure each fold has the correct number of items. [Implementation] Here&#x27;s how you can implement this: ```python def create_folds(data, count): # Return an empty list if count is non-positive if count &lt;= 0: return [] data_length = len(data) fold_size = data_length // count remainder = data_length % count folds = [] start_index = 0 for i in range(count): # Add 1 element to each of the first `remainder` folds end_index = start_index + fold_size + (1 if i &lt; remainder else 0) folds.append(range(start_index, end_index)) start_index = end_index return folds ``` [Explanation] This implementation ensures that each fold has an equal length with at most a difference of 1 between any two folds. It handles edge cases, such as a non-positive `count`, by returning an empty list as specified. [Tests] You can test your code with the provided assertions to verify that the function works correctly: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ## Example 2 ### Instruction Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To create a Python function that creates an XML-like element with a given name, attributes, and children, you can follow these steps: 1. Define a `create_element` function that takes a required &#x27;name&#x27; argument and two optional arguments &#x27;attributes&#x27; and &#x27;children&#x27;. 2. Declare a class `Element` that holds the name, attributes, and children of the element. 3. Inside the `create_element` function, create a new `Element` object with the given &#x27;name&#x27;, &#x27;attributes&#x27;, and &#x27;children&#x27;. 4. Return the newly created `Element` object. 5. Test your function with the given assertions. Here&#x27;s how you can implement this: ```python class Element: def __init__(self, name, attributes=None, children=None): self.name = name self.attributes = attributes or &#123;&#125; self.children = children or [] def create_element(name, attributes=None, children=None): return Element(name, attributes, children) ```&quot;</span><br></code></pre></td></tr></table></figure></p><p>bigest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. Your code should pass the following test case: ```python dictionary = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; new_dict = copy_and_delete(dictionary, &#x27;a&#x27;) assert new_dict == &#123;&#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Design a Python function to encode a list of strings into a unique list with the same length, order, and meaning. The new list is formed by appending asterisks (*) to duplicate strings. For instance, given [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;], the function should return [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;], while for [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] it should return [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] without any changes. Make a precondition check through `assert` to ensure the input is a list of strings. Solve the problem in two lines of code, one for the precondition check and the other for the main logic. ### Response [Reasoning] You can achieve this functionality by first ensuring the input meets your requirements using an `assert` statement with list comprehension. Then, use another list comprehension to iterate through the input list and append `&#x27;*&#x27;` to the strings. The number of `&#x27;*&#x27;` is the number of occurrences of the string before the current index. [Implementation] Here&#x27;s how you can do it in two lines of Python code: ```python def encode_strings(input_list): assert all(isinstance(item, str) for item in input_list), &quot;</span><span class="hljs-type">Input</span> must be a list of strings<span class="hljs-comment">&quot; return [s + &#x27;*&#x27; * input_list[:i].count(s) for i, s in enumerate(input_list)] ``` [Explanation] The `enumerate` function provides both the index and the string in each iteration, allowing you to count the occurrences of the string before the current index. [Tests] Now you can test your function with the provided examples: ```python assert encode_strings([&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;] assert encode_strings([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] ``` Note that although the function is concise, it is not the most efficient solution for large lists, as the `count` method has a time complexity of O(n) for each iteration. For larger lists, a more efficient approach might be to use a dictionary to store the counts of each string and then iterate through the list to append the appropriate number of `&#x27;*&#x27;` to each string. ## Example 2 ### Instruction Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To make a copy of a dictionary, you can use the built-in `dict()` function, which creates a new dictionary from an existing dictionary. To delete a specific key from the dictionary, you can use the `del` keyword or the `pop()` method. Here&#x27;s how you can implement this in Python: ```python def copy_and_delete(dictionary, key): copy_dict = dict(dictionary) if key in copy_dict: del copy_dict[key] return copy_dict ``` The above code makes a copy of the dictionary using `dict()` to create a new dictionary from the existing one. Then, if the specified key exists in the copy, it is deleted from the copy using `del`. The function returns the modified copy.&quot;</span><br></code></pre></td></tr></table></figure></p><p>Difference: Larger ifd instructions are shorter and more abstract,and therefore more difficult to learn?</p><h3id="superfiltering-weak-to-strong-data-filtering-for-fast-instruction-tuning">Superfiltering:Weak-to-Strong Data Filtering for Fast Instruction-Tuning</h3><p>One sentence summary: Demonstrate whether it is feasible to filterfine-tuning data based on small models and then use it for instructionfine-tuning training of large models</p><h5 id="evaluation">Evaluation</h5><p>The main reason why this article and the previous work can work is toensure the consistency of knowledge before and after instructionfine-tuning, that is, before instruction fine-tuning, the model is usedto eliminate the world knowledge that is not part of the model itself(pre-train stage). However, this work did not guarantee that with thepassage of time and the update of later time data, the new LLM wouldstill have the same command difficulty perception ability as the oldmodel such as GPT-2, that is, the sequencing differences of IFD screenedsequences became more and more large.</p><h5 id="abstract-conclusion">Abstract &amp; Conclusion</h5><img src="/2024/10/21/Implementation/4.png" class=""><p>Firstly, the consistency of small model and large model perception ofinstruction complexity is proved, and based on this, an instructionfine-tuning method named "Super filtering" is proposed. Specifically,the small model is used to evaluate the instruction data, and then thefiltered data is used to fine-tune the larger model.</p><p>IFD is still used as an evaluation indicator. Fine-tune the data setfor a given instruction, GPT-2 model and others directly used tocalculate the IFD score of each sample. Then the top k-percent sampleswith the highest IFD scores under 1 are selected for faster instructiontuning.</p><h5 id="design-experiment">Design experiment</h5><ol type="1"><li>Dataset: Alpaca Dataset; Models: GPT-2, GPT-2-Large, GPT-2-XL,GPT-NEO(1.3B), LlaMA3-8B</li><li>Calculate ppl and IFD scores of this dataset on different models andvisualize the results</li><li>Taking the results of LlaMA3-8b as reference, the ppl and IFD scoreranking obtained by other models were calculated with the similarityspearman coefficient ##### Reproduce process</li><li>Calculate ppl and ifd scores for each model on the dataset<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/data_analysis.py \<br>    --data_path data/alpaca_data.json \<br>    --save_path alpaca_data_gpt2_scores.jsonl \<br>    --model_name_or_path gpt2 <br><br>Same with the rest<br></code></pre></td></tr></table></figure></li></ol><p>Starting with xl really takes time...</p><p>Due to limited video memory, a total of three Windows were opened atthe same time, and the final time took about 2.5h Because LlaMA3.1-8B istoo slow, the final result does not include it</p><ol start="2" type="1"><li>Map score back to the original data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/put_analysis_to_data.py \<br>    --pt_data_path alpaca_data_gpt2_scores.jsonl \<br>    --json_data_path data/alpaca_data.json \<br>    --json_save_path 1/alpaca_data_gpt2_data.json<br><br>Same with the rest<br></code></pre></td></tr></table></figure></li></ol><p>This process is fast</p><ol start="3" type="1"><li>Organize the data sequence <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/select_data.py \<br>    --json_data_path 1/alpaca_data_gpt2_data.json \<br>    --json_save_path alpaca_data_gpt2_data_full.json \<br>    --sample_rate 1 \<br>    --filter_threash 2<br><br>Same with the rest<br></code></pre></td></tr></table></figure> Be careful to set threashto prevent ifd&gt;1 from being all shaved clean</li></ol><h5 id="ppl">ppl</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span>  <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ppl_A_condition&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ppl_A_condition&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.9</span>)<br>            lower_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.01</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> lower_limit &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure><p>After repeated experiments, it is found that the ppl diagram in thepaper refers to ppl_A_condition, and the upper limit is 0.9 quantile</p><img src="/2024/10/21/Implementation/5.png" class=""><h5 id="idf">idf</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span> <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ifd_ppl&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.99</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ifd_ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ifd_ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure><p>The upper limit of 0.99 quantile is closer to the experimentalresults of the paper</p><img src="/2024/10/21/Implementation/6.png" class=""><h5 id="spearman">Spearman</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> spearmanr<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt-neo_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data1 = json.load(f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data2 = json.load(f)<br><br>ids1 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data1]<br>ids2 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data2]<br><br>common_ids = <span class="hljs-built_in">set</span>(ids1) &amp; <span class="hljs-built_in">set</span>(ids2)<br><br>ranks1 = [ids1.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]  <br>ranks2 = [ids2.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]<br><br>spearman_corr, p_value = spearmanr(ranks1, ranks2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Spearman Rank Correlation: <span class="hljs-subst">&#123;spearman_corr&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;P-value: <span class="hljs-subst">&#123;p_value&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>neo:1 gpt2: 0.823 gpt2-large: 0.872 gpt-xl: 0.870</p>]]></content>
    
    
    <categories>
      
      <category>PROJECT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PAPER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Project</title>
    <link href="/2024/10/21/Project/"/>
    <url>/2024/10/21/Project/</url>
    
    <content type="html"><![CDATA[<p>This post is a general description of my resume, especially myproject experience.</p><h1 id="education">Education</h1><img src="/2024/10/21/Project/1.png" class=""><h1 id="research-experience">Research Experience</h1><h3 id="data-selection">Data Selection</h3><p>See details in post <ahref="/2024/10/21/Implementation">Implementation of Data FilteringPapers</a></p><h3 id="nus-summer-workshop">NUS Summer Workshop</h3><ul><li>Final grade shows here: <ahref="https://github.com/radicalyyyahaha/NUS-SOC/blob/main/Final%20grade.pdf">Feedback</a></li><li>Github repo: <ahref="https://github.com/radicalyyyahaha/NUS-SOC">2024 NUS</a></li><li>Poster for pre here: <ahref="https://github.com/radicalyyyahaha/NUS-SOC/blob/main/NUS%20final.pdf">POSTER</a></li></ul><p>The program began in May with more than a month of classes followedby several exams. After that, I successfully entered the deep learning&amp; robotics cluster through double selection. The final project is todesign and choose according to the single chip microcomputer. I am theleader of our group, and what our group decided to do is a cyberpetpet.The part I was responsible for was the algorithm programming, which wasmainly to train an algorithm that could track a specific object, andthen after the cyberpet caught the object, it could have the return andonly return the object to the owner, and it also came with a basic voicecommand following system, such as dancing in circles, performing tasks,etc.</p><p>The specific method is to train some object detection algorithms,such as YOLO, DETR (the final decision is YOLO). In order to solve theproblem of inaccurate final detection due to too few training classes,we have made many attempts, such as modifying the model structure andincreasing the data sample size. As for the voice instruction followmodule, we use the existing open source speech2text, and then convertthe voice instructions into digital signals to the MCU forexecution.</p><h3 id="d-panoramic-simulation-for-metaverse">3D Panoramic Simulationfor Metaverse</h3><h3 id="some-other-explorations">Some other explorations:</h3><p>Here are some other small projects I've done, even if I'm not able tofind an repo anymore.(So sad lost them)</p><h3 id="qq-chatbot">QQ chatbot</h3><h3 id="stable-diffusion-so-vits-svc">Stable Diffusion &amp;So-Vits-SVC</h3><h3 id="reinforcement-learning">Reinforcement Learning</h3><h1 id="course-project">Course Project</h1><h3 id="improved-yolo-v8">Improved YOLO v8</h3><h3 id="sound-data-classification">Sound Data Classification</h3><h3 id="heuristic-search">Heuristic Search</h3><h3 id="integrated-design">Integrated Design</h3>]]></content>
    
    
    <categories>
      
      <category>PROJECT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Research Experience, Course Project</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Supervised Contrastive Learning</title>
    <link href="/2024/06/20/Supervised/"/>
    <url>/2024/06/20/Supervised/</url>
    
    <content type="html"><![CDATA[<p>This paper can be considered an improvement over self-supervisedcontrastive learning, offering a method that helps enhance the qualityof features.</p><h3id="introduction-to-self-supervised-contrastive-learning">Introductionto Self-Supervised Contrastive Learning</h3><img src="/2024/06/20/Supervised/1.png" class=""><p>Self-supervised learning involves pre-training a model using a largeamount of unlabeled data to learn data representations. These learnedrepresentations are then used in various downstream tasks, where theyare connected to different classifiers/regressors and trained usinglabeled data from the downstream tasks, making them suitable for thosetasks.</p><p>Essentially, this maps the data onto a high-dimensional hypersphere,where vectors with similar features have high cosine similarity andvectors with dissimilar features are far apart. Word embedding seems toutilize this concept.</p><p>Different models define their losses slightly differently. Here arethe two most classic models:</p><h5 id="simclr">SimCLR</h5><p><span class="math display">\[ L = -\log \frac{\exp(\text{sim}(h_i,h_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \ne i]} \exp(\text{sim}(h_i,h_k)/\tau)} \]</span></p><h5 id="moco">MoCo</h5><p>Momentum Contrast (MoCo) uses a dynamically updated queue to storenegative samples. Its loss function is similar to SimCLR but employs amomentum encoder to generate contrastive representations. <spanclass="math display">\[ L = -\log \frac{\exp(\text{sim}(h_q,h_k^+)/\tau)}{\sum_{i=0}^{K} \exp(\text{sim}(h_q, h_{k_i})/\tau)}\]</span> Where: - $ h_q $ is the representation of the query sample. -$ h_k^+ $ is the representation of the positive sample. - $ h_{k_i} $ isthe representation of the negative samples in the queue.</p><p>While this method can learn good features, it has a shortcoming: itdoes not consider the correlation of features among different imagesbelonging to the same class. <img src="/2024/06/20/Supervised/2.png" class=""></p><h3 id="supervised-contrastive-learning">Supervised ContrastiveLearning</h3><p>To make the features of similar images close to each other, classinformation is used to determine which images belong to the same class.Therefore, the method's name changes from "self-supervised" to"supervised". The basis for contrastive learning shifts from "whetherthey come from the same image" to "whether they belong to the sameclass". The loss function used in training becomes:</p><img src="/2024/06/20/Supervised/3.png" class=""><p>The authors designed two types of loss functions, one with theweighting sum outside the log and one with it inside. Experiments foundthat the first type works better.</p>]]></content>
    
    
    <categories>
      
      <category>PAPER</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SCL, SSCL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kullback-Leibler Divergence</title>
    <link href="/2024/06/20/Divergence/"/>
    <url>/2024/06/20/Divergence/</url>
    
    <content type="html"><![CDATA[<p>Before introducing KL divergence, we need to understand CE lossfirst.</p><h3 id="cross-entropy">Cross-entropy</h3><p>Why not MSE: In statistical learning, when we need to predict aspecific value, such as in a regression model, MSE can be used to definethe loss. However, if the final prediction is to select one frommultiple classes, i.e., the prediction results will be probabilitiesdistributed across different classes, we need to use cross-entropyloss.</p><p>For the general definition of entropy, we usually consider:</p><p><span class="math display">\[H=-\sum P(X)\log P(X)\]</span></p><p>Where <span class="math inline">\(P(x)\)</span> is the probability(frequency) of the event occurring, <span class="math inline">\(-\logP(x_i) = \log \frac{1}{P(x_i)}\)</span>, can be understood as the numberof occurrences of the event within a unit statistical count. Thisformula well demonstrates the nature of entropy: when an event is verycertain (probability close to 0 or 1), its entropy is close to 0;conversely, if we are unsure, such as flipping a fair coin, the entropyis larger.</p><p>As for cross-entropy, as the name suggests, when the true probabilitydistribution of an event is <span class="math inline">\(P(X)\)</span>,and we use an approximate distribution (Q(X)) to fit it, the entropy atthis time should be:</p><p><span class="math display">\[H(P, Q)=-\sum P(X)\log Q(X)\]</span></p><p>In the cross-entropy loss function, <spanclass="math inline">\(P(X)\)</span> generally contains the one-hotencoding of the true labels. For example, for binary classificationproblems, the well-known BCELoss() should be:</p><p><spanclass="math display">\[BCE=-y\log(\hat{y})-(1-y)\log(1-\hat{y})\]</span></p><h3 id="kl-divergence">KL Divergence</h3><p>KL divergence is used to measure the difference between probabilitydistributions. More specifically, it describes the amount of informationloss when we use <span class="math inline">\(Q(X)\)</span> to fit <spanclass="math inline">\(P(X)\)</span>:</p><p><span class="math display">\[D_{KL}(P||Q)=H(P,Q)-H(P)=\sumP(X)\log\frac{P(X)}{Q(X)}\]</span></p><p>Clearly, KL divergence is not symmetric, and therefore it is not ametric.</p><p>Noting that if <span class="math inline">\(g\)</span> is anyreal-valued measurable function, and <spanclass="math inline">\(\varphi\)</span> is a convex function within therange of <span class="math inline">\(g\)</span>, then</p><figure><imgsrc="https://wikimedia.org/api/rest_v1/media/math/render/svg/d33a0b42492bc7a9c197ec7c8717f5a073236f5e"alt="{({-}^{}g(x)f(x),dx){-}^{}(g(x))f(x),dx.}" /><figcaptionaria-hidden="true">{(<em>{-}^{}g(x)f(x),dx)</em>{-}^{}(g(x))f(x),dx.}</figcaption></figure><p>Using this property, it is not difficult to find:</p><p><span class="math display">\[D_{KL}(P||Q)=\intP(X)\log\frac{P(X)}{Q(X)} \ge -\log\int Q(X) =0 \]</span></p><p>Example: Calculating KL divergence between two multivariate Gaussiandistributions: <img src="/2024/06/20/Divergence/1.jpg" class=""></p>]]></content>
    
    
    <categories>
      
      <category>MATH</category>
      
    </categories>
    
    
    <tags>
      
      <tag>KL, CE</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

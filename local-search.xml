<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>task2</title>
    <link href="/2025/04/10/task2/"/>
    <url>/2025/04/10/task2/</url>
    
    <content type="html"><![CDATA[<p><strong>Survival Analysis Report on Telco Customer Churn</strong></p><p><strong>Overview</strong> This report outlines the process andinsights gained from performing <strong>survival analysis</strong> onthe Telco Customer Churn dataset. The goal of this analysis is tounderstand <strong>customer retention</strong> and <strong>churnbehavior over time</strong>, specifically identifying how long customerstend to stay with the company and what factors influence theirdeparture.</p><p><strong>1. Objective</strong></p><p>• Estimate the <strong>survival probability</strong> of customersover time.</p><p>• Identify <strong>factors influencing churn</strong>, such ascontract type, tenure, payment method, and service usage.</p><p>• Visualize survival curves for different segments of customers(e.g., by internet service type or gender).</p><p>• Provide actionable insights for improving retention.</p><p><strong>2. Dataset Description</strong> We use the <strong>silverlayer</strong> of the Telco Customer Churn dataset, which containsmonthly customers with internet service. This layer was created from rawdata through a PySpark ETL pipeline.</p><p><strong>Key Columns for Analysis:</strong></p><p>• tenure — Duration (in months) the customer has stayed.</p><p>• churn — Binary indicator of churn (1 if churned, 0 otherwise).</p><p>• contract, internetService, paymentMethod, etc. — Categoricalfeatures for subgroup analysis.</p><p><strong>3. Methodology</strong></p><p><strong>3.1 Data Preparation</strong></p><p>• The data was loaded from a Delta Lake “silver” table.</p><p>• Records with missing or invalid values were filtered.</p><p>• The churn column was used as the <strong>eventindicator</strong>.</p><p>• The tenure column was used as the <strong>duration untilchurn</strong>.</p><p><strong>3.2 Survival Analysis Tools</strong></p><p>• <strong>Kaplan-Meier Estimator</strong>: Used to calculate andvisualize survival probabilities over time.</p><p>• <strong>Log-Rank Test</strong>: Used to compare survival curvesbetween groups.</p><p>• Optional: <strong>Cox Proportional Hazards Model</strong> formultivariate analysis (not covered here but can be added).</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Implementation</title>
    <link href="/2024/10/21/Implementation/"/>
    <url>/2024/10/21/Implementation/</url>
    
    <content type="html"><![CDATA[<h1 id="overview">Overview</h1><p>In the early research, the work of instruction tuning is mainlyfocused on the construction of large-scale instruction data sets, andthere are two main ways to create instruction data sets. One is toconvert a text-label pair from an existing annotated natural languagedataset to an instruction-output pair, such as P3, via a template.Another way is to use an LLM such as GPT-3.5-Turbo to generate outputfor a given instruction. While instruction fine-tuning relies primarilyon large amounts of data, LIMA and others have shown that data qualityis more critical than quantity. They demonstrated a significant increasein LLM performance using just 1k high-quality instruction data. Thisfinding suggests that LLM has acquired world knowledge in thepre-training phase and requires only a small amount of high-qualityinstruction data to generate high-quality responses in the instructiontuning phase.</p><p>Various instruction tuning datasets generated by LLMS, such asSelf-Instruct and Alpaca, provide large samples without manualmanipulation, but their data quality depends on the LLM's performanceand is uncertain. In contrast, hand-collated datasets, such as LIMA andDolly, achieve higher quality through human selection, but may besubject to human bias. Alternative data set construction methods, suchas prompt mapping and mapping, aim to improve data set quality anddiversity, but present new challenges in terms of quality assurance.This variability in data set construction and source can significantlyaffect data quality, highlighting the importance of careful dataselection for LLM instruction tuning.</p><p>The methods can be divided into the following four categories:methods based on system of indicators, trainable LLMs, powerful LLMslike ChatGPT and small models.</p><p>The effectiveness of a data selection method depends on the qualityof the selected subset from a given data set. In order to measure thequality of subsets, LLMS fine-tuned on subsets are evaluated ondifferent benchmarks by different methods, which can be divided intothree categories: wining rate, inner comparison and externalcomparison.</p><ul><li><a href="https://arxiv.org/abs/2402.05123">A Survey on DataSelection for LLM Instruction Tuning</a></li><li><a href="https://arxiv.org/abs/2402.16827">A Survey on DataSelection for Language Models</a></li></ul><h3id="from-quantity-to-quality-boosting-llm-performance-with-self-guided-data-selection-for-instruction-tuning">FromQuantity to Quality: Boosting LLM Performance with Self-Guided DataSelection for Instruction Tuning</h3><p>One sentence summary: Let the model choose the data for itself</p><h5 id="design-experiment">Design experiment</h5><ol type="1"><li>Use k-means to filter out 1k instructions to fine-tune the data fortraining and train only one epoch</li><li>This cherry-model is then used to filter the data in reverse, usingthe IFD metric</li><li><span class="math display">\[IFD_\theta(Q,A)=\frac{s_\theta(A|Q)}{s_\theta(A)}\]</span></li></ol><img src="/2024/10/21/Implementation/1.png" class=""><ol start="4" type="1"><li>This formula selects data with IFD less than 1 and a high ratio,which means that these data are difficult for the model to learn</li><li>Then use this part to filter out more valuable training data totrain the model</li></ol><h5 id="reproduce-process">Reproduce process</h5><p>Use configuration: <img src="/2024/10/21/Implementation/2.png" class=""> 1. Select 1k pieces of data<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_pre.pt \<br>    --model_name_or_path &quot;meta-llama/Llama-3.1-8B&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod pre<br></code></pre></td></tr></table></figure></p><p>--data_path: The targeted dataset in the Alpaca format</p><p>--save_path: The path to save the .pt file containing embeddings orscores</p><p>--prompt: The prompt type used for training and selecting data, canchoose between alpaca or wiz</p><p>Change in <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">        <span class="hljs-keyword">elif</span> args.prompt == <span class="hljs-string">&#x27;code&#x27;</span>:<br>            input_i = data_i[<span class="hljs-string">&#x27;prompt&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;prompt&#x27;</span> <span class="hljs-keyword">in</span> data_i.keys() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span><br>            <span class="hljs-keyword">if</span> input_i == <span class="hljs-string">&#x27;&#x27;</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_no_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>            <span class="hljs-keyword">else</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i,<span class="hljs-string">&#x27;prompt&#x27;</span>:input_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>                <br>           <br><span class="hljs-keyword">and</span> some other replacement of <span class="hljs-built_in">input</span>-prompt, output-response<br></code></pre></td></tr></table></figure></p><p>--mod: pre used for getting needed embeddings or scores on selectingpre-experienced samples and cherry used for cherry</p><img src="/2024/10/21/Implementation/3.png" class=""><p>Some parameters are on the meta device because they were offloaded tothe cpu. Reason: Probably because of insufficient GPU memory</p><p>It was estimated to be about 25min, but it was actually very, verylong... But The author also admitted it in limitation. the mainlimitation of this method is the inconvenience of training thepre-experienced model.</p><p>So I randomly selected 1k pieces of data to feel ~ Time Used:27.98344091176987 (min)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_cluster.py \<br>    --pt_data_path code_data_pre.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_pre.json \<br>    --sample_num 8 \<br>    --kmeans_num_clusters 100 \<br>    --low_th 25 \<br>    --up_th 75<br></code></pre></td></tr></table></figure><p>--pt_data_path: The .pt file from previous step containing neededembeddings or scores</p><p>--json_data_path: The targeted dataset in the Alpaca format</p><p>--json_save_path: The path to save the selected pre-experiencedsamples</p><p>--sample_num: How many samples will be selected in each cluster</p><p>--kmeans_num_clusters: How many clusters will be generated byK-Means</p><p>--low_th and --up_th: The lower and Upper threshold for selectingsamples within each cluster</p><p>In an instant, 420 items were selected (why 420 items? Is it becauseeach cluster is not selected?)</p><ol start="2" type="1"><li>Train Pre-Experienced Model <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">!pip install unsloth<br></code></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> FastLanguageModel<br><span class="hljs-keyword">import</span> torch<br>max_seq_length = <span class="hljs-number">2048</span> <span class="hljs-comment"># Choose any! We auto support RoPE Scaling internally!</span><br>dtype = <span class="hljs-literal">None</span> <span class="hljs-comment"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span><br>load_in_4bit = <span class="hljs-literal">True</span> <span class="hljs-comment"># Use 4bit quantization to reduce memory usage. Can be False.</span><br><br><br>model, tokenizer = FastLanguageModel.from_pretrained(<br>    model_name = <span class="hljs-string">&quot;unsloth/Meta-Llama-3.1-8B&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dtype = dtype,<br>    load_in_4bit = load_in_4bit,<br>    <span class="hljs-comment"># token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span><br>)<br><br><br>model = FastLanguageModel.get_peft_model(<br>    model,<br>    r = <span class="hljs-number">16</span>, <span class="hljs-comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span><br>    target_modules = [<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>,<br>                      <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>,],<br>    lora_alpha = <span class="hljs-number">16</span>,<br>    lora_dropout = <span class="hljs-number">0</span>, <span class="hljs-comment"># Supports any, but = 0 is optimized</span><br>    bias = <span class="hljs-string">&quot;none&quot;</span>,    <span class="hljs-comment"># Supports any, but = &quot;none&quot; is optimized</span><br>    <span class="hljs-comment"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span><br>    use_gradient_checkpointing = <span class="hljs-string">&quot;unsloth&quot;</span>, <span class="hljs-comment"># True or &quot;unsloth&quot; for very long context</span><br>    random_state = <span class="hljs-number">3407</span>,<br>    use_rslora = <span class="hljs-literal">False</span>,  <span class="hljs-comment"># We support rank stabilized LoRA</span><br>    loftq_config = <span class="hljs-literal">None</span>, <span class="hljs-comment"># And LoftQ</span><br>)<br><br><br>alpaca_prompt = <span class="hljs-string">&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Instruction:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Input:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Response:</span><br><span class="hljs-string">&#123;&#125;&quot;&quot;&quot;</span><br><br>EOS_TOKEN = tokenizer.eos_token <span class="hljs-comment"># Must add EOS_TOKEN</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">formatting_prompts_func</span>(<span class="hljs-params">examples</span>):<br>    instructions = examples[<span class="hljs-string">&quot;instruction&quot;</span>]<br>    inputs       = examples[<span class="hljs-string">&quot;prompt&quot;</span>]<br>    outputs      = examples[<span class="hljs-string">&quot;response&quot;</span>]<br>    texts = []<br>    <span class="hljs-keyword">for</span> instruction, <span class="hljs-built_in">input</span>, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(instructions, inputs, outputs):<br>        <span class="hljs-comment"># Must add EOS_TOKEN, otherwise your generation will go on forever!</span><br>        text = alpaca_prompt.<span class="hljs-built_in">format</span>(instruction, <span class="hljs-built_in">input</span>, output) + EOS_TOKEN<br>        texts.append(text)<br>    <span class="hljs-keyword">return</span> &#123; <span class="hljs-string">&quot;text&quot;</span> : texts, &#125;<br><span class="hljs-keyword">pass</span><br><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;code_data_pre.json&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = dataset.<span class="hljs-built_in">map</span>(formatting_prompts_func, batched = <span class="hljs-literal">True</span>,)<br><br><br><br><span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTTrainer<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> is_bfloat16_supported<br><br>trainer = SFTTrainer(<br>    model = model,<br>    tokenizer = tokenizer,<br>    train_dataset = dataset,<br>    dataset_text_field = <span class="hljs-string">&quot;text&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dataset_num_proc = <span class="hljs-number">2</span>,<br>    packing = <span class="hljs-literal">False</span>, <span class="hljs-comment"># Can make training 5x faster for short sequences.</span><br>    args = TrainingArguments(<br>        per_device_train_batch_size = <span class="hljs-number">2</span>,<br>        gradient_accumulation_steps = <span class="hljs-number">4</span>,<br>        warmup_steps = <span class="hljs-number">5</span>,<br>        num_train_epochs = <span class="hljs-number">1</span>, <span class="hljs-comment"># Set this for 1 full training run.</span><br>        <span class="hljs-comment">#max_steps = 60,</span><br>        learning_rate = <span class="hljs-number">2e-4</span>,<br>        fp16 = <span class="hljs-keyword">not</span> is_bfloat16_supported(),<br>        bf16 = is_bfloat16_supported(),<br>        logging_steps = <span class="hljs-number">1</span>,<br>        optim = <span class="hljs-string">&quot;adamw_8bit&quot;</span>,<br>        weight_decay = <span class="hljs-number">0.01</span>,<br>        lr_scheduler_type = <span class="hljs-string">&quot;linear&quot;</span>,<br>        seed = <span class="hljs-number">3407</span>,<br>        output_dir = <span class="hljs-string">&quot;outputs&quot;</span>,<br>    ),<br>)<br><br>trainer.train()<br></code></pre></td></tr></table></figure><p>Time Used: 8:03min</p><ol start="3" type="1"><li>通过IFD选cherry data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_cherry.pt \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod cherry<br></code></pre></td></tr></table></figure> Time Used: 6.185897000630697(min)</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_IFD.py \<br>    --pt_data_path code_data_cherry.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_cherry.json \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 2048 \<br>    --sample_rate 0.06 \<br>    --prompt code<br></code></pre></td></tr></table></figure><p>--sample_rate: How many cherry samples you would like to select? Youcan also use --sample_number to set the exact number of samples.Finally, 60 pieces of data with IFD&lt;1 were obtained, and soted sortedthem from smallest to largest</p><p>Smallest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` Your code should pass the following test case: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) assert elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Construct a Python function `create_folds(data, count)` to create a list of folds from the given data, where each fold is a subsequence of the original data. The length of each fold should be approximately equal with at most a difference of 1. The function should satisfy the following assertion: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ### Response [Reasoning] To create a Python function that generates a list of folds from given data with each fold having approximately equal length (with at most a difference of 1 between any two folds), you can follow these steps: 1. Return an empty list immediately if `count` is non-positive since it&#x27;s not possible to create a valid number of folds. 2. Divide the total length of the data by `count`, rounding down, to get the minimum number of items each fold should have. 3. Calculate the remainder to see how many extra items need to be distributed among the folds. They are distributed one by one to the first `remainder` folds. 4. Use a loop to create each fold, adjusting the starting and ending indices based on the calculations above to ensure each fold has the correct number of items. [Implementation] Here&#x27;s how you can implement this: ```python def create_folds(data, count): # Return an empty list if count is non-positive if count &lt;= 0: return [] data_length = len(data) fold_size = data_length // count remainder = data_length % count folds = [] start_index = 0 for i in range(count): # Add 1 element to each of the first `remainder` folds end_index = start_index + fold_size + (1 if i &lt; remainder else 0) folds.append(range(start_index, end_index)) start_index = end_index return folds ``` [Explanation] This implementation ensures that each fold has an equal length with at most a difference of 1 between any two folds. It handles edge cases, such as a non-positive `count`, by returning an empty list as specified. [Tests] You can test your code with the provided assertions to verify that the function works correctly: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ## Example 2 ### Instruction Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To create a Python function that creates an XML-like element with a given name, attributes, and children, you can follow these steps: 1. Define a `create_element` function that takes a required &#x27;name&#x27; argument and two optional arguments &#x27;attributes&#x27; and &#x27;children&#x27;. 2. Declare a class `Element` that holds the name, attributes, and children of the element. 3. Inside the `create_element` function, create a new `Element` object with the given &#x27;name&#x27;, &#x27;attributes&#x27;, and &#x27;children&#x27;. 4. Return the newly created `Element` object. 5. Test your function with the given assertions. Here&#x27;s how you can implement this: ```python class Element: def __init__(self, name, attributes=None, children=None): self.name = name self.attributes = attributes or &#123;&#125; self.children = children or [] def create_element(name, attributes=None, children=None): return Element(name, attributes, children) ```&quot;</span><br></code></pre></td></tr></table></figure></p><p>bigest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. Your code should pass the following test case: ```python dictionary = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; new_dict = copy_and_delete(dictionary, &#x27;a&#x27;) assert new_dict == &#123;&#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Design a Python function to encode a list of strings into a unique list with the same length, order, and meaning. The new list is formed by appending asterisks (*) to duplicate strings. For instance, given [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;], the function should return [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;], while for [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] it should return [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] without any changes. Make a precondition check through `assert` to ensure the input is a list of strings. Solve the problem in two lines of code, one for the precondition check and the other for the main logic. ### Response [Reasoning] You can achieve this functionality by first ensuring the input meets your requirements using an `assert` statement with list comprehension. Then, use another list comprehension to iterate through the input list and append `&#x27;*&#x27;` to the strings. The number of `&#x27;*&#x27;` is the number of occurrences of the string before the current index. [Implementation] Here&#x27;s how you can do it in two lines of Python code: ```python def encode_strings(input_list): assert all(isinstance(item, str) for item in input_list), &quot;</span><span class="hljs-type">Input</span> must be a list of strings<span class="hljs-comment">&quot; return [s + &#x27;*&#x27; * input_list[:i].count(s) for i, s in enumerate(input_list)] ``` [Explanation] The `enumerate` function provides both the index and the string in each iteration, allowing you to count the occurrences of the string before the current index. [Tests] Now you can test your function with the provided examples: ```python assert encode_strings([&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;] assert encode_strings([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] ``` Note that although the function is concise, it is not the most efficient solution for large lists, as the `count` method has a time complexity of O(n) for each iteration. For larger lists, a more efficient approach might be to use a dictionary to store the counts of each string and then iterate through the list to append the appropriate number of `&#x27;*&#x27;` to each string. ## Example 2 ### Instruction Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To make a copy of a dictionary, you can use the built-in `dict()` function, which creates a new dictionary from an existing dictionary. To delete a specific key from the dictionary, you can use the `del` keyword or the `pop()` method. Here&#x27;s how you can implement this in Python: ```python def copy_and_delete(dictionary, key): copy_dict = dict(dictionary) if key in copy_dict: del copy_dict[key] return copy_dict ``` The above code makes a copy of the dictionary using `dict()` to create a new dictionary from the existing one. Then, if the specified key exists in the copy, it is deleted from the copy using `del`. The function returns the modified copy.&quot;</span><br></code></pre></td></tr></table></figure></p><p>Difference: Larger ifd instructions are shorter and more abstract,and therefore more difficult to learn?</p><h3id="superfiltering-weak-to-strong-data-filtering-for-fast-instruction-tuning">Superfiltering:Weak-to-Strong Data Filtering for Fast Instruction-Tuning</h3><p>One sentence summary: Demonstrate whether it is feasible to filterfine-tuning data based on small models and then use it for instructionfine-tuning training of large models</p><h5 id="evaluation">Evaluation</h5><p>The main reason why this article and the previous work can work is toensure the consistency of knowledge before and after instructionfine-tuning, that is, before instruction fine-tuning, the model is usedto eliminate the world knowledge that is not part of the model itself(pre-train stage). However, this work did not guarantee that with thepassage of time and the update of later time data, the new LLM wouldstill have the same command difficulty perception ability as the oldmodel such as GPT-2, that is, the sequencing differences of IFD screenedsequences became more and more large.</p><h5 id="abstract-conclusion">Abstract &amp; Conclusion</h5><img src="/2024/10/21/Implementation/4.png" class=""><p>Firstly, the consistency of small model and large model perception ofinstruction complexity is proved, and based on this, an instructionfine-tuning method named "Super filtering" is proposed. Specifically,the small model is used to evaluate the instruction data, and then thefiltered data is used to fine-tune the larger model.</p><p>IFD is still used as an evaluation indicator. Fine-tune the data setfor a given instruction, GPT-2 model and others directly used tocalculate the IFD score of each sample. Then the top k-percent sampleswith the highest IFD scores under 1 are selected for faster instructiontuning.</p><h5 id="design-experiment-1">Design experiment</h5><ol type="1"><li>Dataset: Alpaca Dataset; Models: GPT-2, GPT-2-Large, GPT-2-XL,GPT-NEO(1.3B), LlaMA3-8B</li><li>Calculate ppl and IFD scores of this dataset on different models andvisualize the results</li><li>Taking the results of LlaMA3-8b as reference, the ppl and IFD scoreranking obtained by other models were calculated with the similarityspearman coefficient ##### Reproduce process</li><li>Calculate ppl and ifd scores for each model on the dataset<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/data_analysis.py \<br>    --data_path data/alpaca_data.json \<br>    --save_path alpaca_data_gpt2_scores.jsonl \<br>    --model_name_or_path gpt2 <br><br>Same with the rest<br></code></pre></td></tr></table></figure></li></ol><p>Starting with xl really takes time...</p><p>Due to limited video memory, a total of three Windows were opened atthe same time, and the final time took about 2.5h Because LlaMA3.1-8B istoo slow, the final result does not include it</p><ol start="2" type="1"><li>Map score back to the original data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/put_analysis_to_data.py \<br>    --pt_data_path alpaca_data_gpt2_scores.jsonl \<br>    --json_data_path data/alpaca_data.json \<br>    --json_save_path 1/alpaca_data_gpt2_data.json<br><br>Same with the rest<br></code></pre></td></tr></table></figure></li></ol><p>This process is fast</p><ol start="3" type="1"><li>Organize the data sequence <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/select_data.py \<br>    --json_data_path 1/alpaca_data_gpt2_data.json \<br>    --json_save_path alpaca_data_gpt2_data_full.json \<br>    --sample_rate 1 \<br>    --filter_threash 2<br><br>Same with the rest<br></code></pre></td></tr></table></figure> Be careful to set threashto prevent ifd&gt;1 from being all shaved clean</li></ol><h5 id="ppl">ppl</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span>  <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ppl_A_condition&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ppl_A_condition&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.9</span>)<br>            lower_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.01</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> lower_limit &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure><p>After repeated experiments, it is found that the ppl diagram in thepaper refers to ppl_A_condition, and the upper limit is 0.9 quantile</p><img src="/2024/10/21/Implementation/5.png" class=""><h5 id="idf">idf</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span> <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ifd_ppl&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.99</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ifd_ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ifd_ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure><p>The upper limit of 0.99 quantile is closer to the experimentalresults of the paper</p><img src="/2024/10/21/Implementation/6.png" class=""><h5 id="spearman">Spearman</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> spearmanr<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt-neo_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data1 = json.load(f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data2 = json.load(f)<br><br>ids1 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data1]<br>ids2 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data2]<br><br>common_ids = <span class="hljs-built_in">set</span>(ids1) &amp; <span class="hljs-built_in">set</span>(ids2)<br><br>ranks1 = [ids1.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]  <br>ranks2 = [ids2.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]<br><br>spearman_corr, p_value = spearmanr(ranks1, ranks2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Spearman Rank Correlation: <span class="hljs-subst">&#123;spearman_corr&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;P-value: <span class="hljs-subst">&#123;p_value&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>neo:1 gpt2: 0.823 gpt2-large: 0.872 gpt-xl: 0.870</p><h3id="less-selecting-influential-data-for-targeted-instruction-tuning">LESS:Selecting Influential Data for Targeted Instruction Tuning</h3><p>One sentence summary: Perform a small amount of tuning on thetraining set as warmup, then calculate the influence among all thetraining data on the verification set, and finally select the data withthe largest impact factor for model training.</p><p>Evaluation: The computational overhead is a little too high.</p><h5 id="design-experiment-2">Design experiment</h5><img src="/2024/10/21/Implementation/9.png" class=""><p>Step 1: Warmup training with LoRA Efficient fine-tuning of N epochesof FM on a random subset of a training set using LoRA yields low-rankparameters while saving model parameters (ckpt) after each epoch.</p><p>Step 2: Projecting the gradients For each training data, calculateits gradient in step 1, use RP (random projection) to generatelow-dimensional gradient features, and store these features in thegradient datastore.</p><p>Step 3: Data Selection Algorithm For the val set of the target task,for each subtask D, use the parameters <spanclass="math display">\[\theta_1, \theta_2,..., \theta_N \]</span>tocalculate the average gradient feature <img src="/2024/10/21/Implementation/7.png" class=""></p><p>Then the data is selected according to the similarity between thegradient features of the data points and the verification set<img src="/2024/10/21/Implementation/8.png" class=""></p><h5 id="reproduce-process-1">Reproduce process</h5><p>Step 1: warm up training <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">DATA_DIR=./data \<br>MODEL_PATH=../hub/shakechen/Llama-2-7b-hf \<br>PERCENTAGE=0.01 \<br>DATA_SEED=3 \<br>JOB_NAME=llama2-7b-p$&#123;PERCENTAGE&#125;-lora-seed$&#123;DATA_SEED&#125; \<br><br>./less/scripts/train/warmup_lora_train.sh &quot;$DATA_DIR&quot; &quot;$MODEL_PATH&quot; &quot;$PERCENTAGE&quot; &quot;$DATA_SEED&quot; &quot;$JOB_NAME&quot;<br></code></pre></td></tr></table></figure></p><p>After running Step2, the optimizer.bin file is missing and onlyoptimizer.pt is found in the ckpt folder, check out the issues:<img src="/2024/10/21/Implementation/10.png" class=""></p><p>then run again. Cost: 3:13h on 3 A40-48G</p><p>Step 2: Building the gradient datastore <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs script">CKPT=12<br><br>TRAINING_DATA_NAME=dolly<br>TRAINING_DATA_FILE=./data/dolly_data.jsonl # when changing data name, change the data path accordingly<br>GRADIENT_TYPE=&quot;adam&quot;<br>MODEL_PATH=../out/llama2-7b-p0.01-lora-seed3/checkpoint-$&#123;CKPT&#125;<br>OUTPUT_PATH=../grads/llama2-7b-p0.01-lora-seed3/$&#123;TRAINING_DATA_NAME&#125;-ckpt$&#123;CKPT&#125;-$&#123;GRADIENT_TYPE&#125;<br>DIMS=&quot;8192&quot;<br><br>./less/scripts/get_info/grad/get_train_lora_grads.sh &quot;$TRAINING_DATA_FILE&quot; &quot;$MODEL_PATH&quot; &quot;$OUTPUT_PATH&quot; &quot;$DIMS&quot; &quot;$GRADIENT_TYPE&quot;<br></code></pre></td></tr></table></figure> Cost:43:56min</p><p>Step 3: Selecting data for a task <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">CKPT=12<br>TASK=mmlu<br>MODEL_PATH=../out/llama2-7b-p0.01-lora-seed3/checkpoint-$&#123;CKPT&#125;<br>OUTPUT_PATH=../grads/llama2-7b-p0.01-lora-seed3/$&#123;TASK&#125;-ckpt$&#123;CKPT&#125;-sgd # for validation data, we always use sgd<br>DATA_DIR=../data<br>DIMS=&quot;4096 8192&quot; # We use 8192 as our default projection dimension <br><br>./less/scripts/get_info/grad/get_eval_lora_grads.sh &quot;$TASK&quot; &quot;$DATA_DIR&quot; &quot;$MODEL_PATH&quot; $OUTPUT_PATH &quot;$DIMS&quot;<br></code></pre></td></tr></table></figure></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs script">DIM=8192 # decide which dimension to use<br>GRADIENT_PATH=../grads/llama2-7b-p0.01-lora-seed3/dolly-ckpt12-adam/dim$&#123;DIM&#125;<br>TRAIN_FILE_NAMES=&quot;dolly&quot;<br>CKPTS=&quot;12&quot; # checkpoing index<br>CHECKPOINT_WEIGHTS=&quot;1.6877e-05&quot; # average lr of the epoch<br><br>VALIDATION_GRADIENT_PATH=../grads/llama2-7b-p0.01-lora-seed3/mmlu-ckpt12-sgd/dim$&#123;DIM&#125;<br>TARGET_TASK_NAMES=&quot;mmlu&quot;<br>SELECTED_DATA_OUTPUT_PATH=&quot;../selected_data&quot;<br><br>./less/scripts/data_selection/matching.sh &quot;$GRADIENT_PATH&quot; &quot;$TRAIN_FILE_NAMES&quot; &quot;$CKPTS&quot; &quot;$CHECKPOINT_WEIGHTS&quot; &quot;$VALIDATION_GRADIENT_PATH&quot; &quot;$TARGET_TASK_NAMES&quot; &quot;$SELECTED_DATA_OUTPUT_PATH&quot;<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python3 -m less.data_selection.write_selected_data \<br>--target_task_names $&#123;TARGET_TASK_NAMES&#125; \<br>--train_file_names $&#123;TRAIN_FILE_NAMES&#125; \<br>--train_files ../data/train/processed/dolly/dolly_data.jsonl \<br>--output_path $SELECTED_DATA_OUTPUT_PATH \<br>--percentage 0.01<br></code></pre></td></tr></table></figure><p>dataset: dolly, choose top 10 data:</p><table><thead><tr class="header"><th>rank</th><th>file name</th><th>index</th><th>score</th></tr></thead><tbody><tr class="odd"><td>1</td><td>dolly</td><td>11921</td><td>0.073349</td></tr><tr class="even"><td>2</td><td>dolly</td><td>10117</td><td>0.072425</td></tr><tr class="odd"><td>3</td><td>dolly</td><td>7017</td><td>0.071267</td></tr><tr class="even"><td>4</td><td>dolly</td><td>14825</td><td>0.07023</td></tr><tr class="odd"><td>5</td><td>dolly</td><td>4947</td><td>0.068469</td></tr><tr class="even"><td>6</td><td>dolly</td><td>13839</td><td>0.067585</td></tr><tr class="odd"><td>7</td><td>dolly</td><td>2031</td><td>0.067453</td></tr><tr class="even"><td>8</td><td>dolly</td><td>13638</td><td>0.067247</td></tr><tr class="odd"><td>9</td><td>dolly</td><td>7766</td><td>0.067236</td></tr><tr class="even"><td>10</td><td>dolly</td><td>11697</td><td>0.067214</td></tr></tbody></table><h3 id="deita-data-efficient-instruction-tuning-for-alignment">Deita:Data-Efficient Instruction Tuning for Alignment</h3><p>One sentence summary: The data were screened by scoring from threedimensions: complexity, quality and diversity</p><h5 id="abstractconclusion">abstract&amp;conclusion</h5><img src="/2024/10/21/Implementation/11.png" class=""><img src="/2024/10/21/Implementation/12.png" class=""><h5 id="design-experiment-3">Design experiment</h5><p>Two kinds of data were collected, one was SOTA model data, with 300Kpieces, and the other was some ordinary base data, with 100K pieces.</p><ol type="1"><li><p>Evol Complexity Given a small seed dataset <spanclass="math display">\[D={(I_1^{(0)},R_1^{(0)}),... , (I_N ^ {} (0), R_N^ {(0)})} \]</span>, for each command <span class="math display">\[I_k\]</span>^ 0, increase the complexity of the instruction (increaseconditions, deepen instructions, specific instructions, improvereasoning step). After M iterations, instructions of differentcomplexity can be obtained, <span class="math display">\[I_k^{(0)},...,I_k^{(M)}\]</span>, and then use ChatGPT to sort and score these Minstructions.</p></li><li><p>Evol Quality The approach is basically the same, except that thegenerated responses are used to let ChatGPT score the instructiondata.</p></li><li><p>Diversity Firstly, the data in the data pool is sorted indescending order according to the comprehensive score of complexity andquality (complexity score * quality score), and then sample data <spanclass="math display">\[x_i\]</span>is extracted one by one in order tocalculate the distance value between <spanclass="math display">\[x_i\]</span>and the nearest sample in thescreening pool. In this way, the data is represented by vector using theLlama1-13B model. The distance is calculated using cosine similarity. Ifthe distance value is less than τ, it is considered that the sample isnot highly similar to the data in the screening pool and can be includedin the screening pool. Otherwise, it is not included in the screeningpool. When the number of samples in the screening pool reaches thespecified number of samples, the diversity screening is completed.<img src="/2024/10/21/Implementation/13.png" class=""></p></li></ol><p>Repetition reference site:https://distilabel.argilla.io/1.0.2/sections/papers/deita/</p><h3 id="mods-model-oriented-data-selection-for-instruction-tuning">MoDS:Model-oriented Data Selection for Instruction Tuning</h3><p>One sentence summary: A data filtering method based on the model'sown perception of instruction data</p><p>evaluate: It's a little too cumbersome, and the LLM needs to befine-tuned to be evaluated and is already a waste of time.</p><h5 id="design-experiment-4">Design experiment</h5><img src="/2024/10/21/Implementation/14.png" class=""><p>Step1: Utilize the reward-model-debertav3-large-v2 model which isdeveloped by OpenAssistant to score the triplet(Instruction, input,output), obtains a score, when the score exceeds α, the data quality isconsidered to meet the standard, and a high-quality dataset isconstructed.</p><p>Step2: The K-Center-Greedy algorithm is used to achieve diversityscreening <img src="/2024/10/21/Implementation/15.png" class=""></p><p>Step3: The data set selected in the second step is used for initialfine-tuning of the model. After obtaining the following ability of basicinstructions, the high-quality data set of the first step is used forreasoning, and the result is scored by the reward model. When the scoreis less than β, it indicates that the initial model cannot generatehigh-quality responses to these instructions and does not have theability to process these types of instructions, so as to obtain thenecessary data set. Then, the necessity Data set row diversity isfiltered to obtain Augmented Instruction Data.</p><p>Step4: Merge these two datasets(Augmented datasets and seeddatasets), and then finetune the raw pre-trained LLM.</p><h5 id="reproduce-process-2">Reproduce process</h5><p>Step1: Use reward models to filter high-quality data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python  ./quality-evaluation/quality-evaluation.py ./alpaca_data.json ./quality-evaluation.json<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./quality-evaluation/high-quality-data-extraction.py ./quality-evaluation.json ./high-quality-data.json 0.0<br></code></pre></td></tr></table></figure></p><p>The author made a small error that if run directly will reportTypeError: '&gt;' not supported between instances of 'float' and 'str',line 103 of high-quality-data-extraction.py needs to be modified asfollows: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">float</span>(item[<span class="hljs-string">&#x27;reward_score&#x27;</span>]) &gt; <span class="hljs-built_in">float</span>(threshold):<br></code></pre></td></tr></table></figure></p><p>Step2: Filter diversity data top_k = 500 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./diverse-data-selection/run.py ./high-quality-data.json ./seed-instructions.json top_k<br></code></pre></td></tr></table></figure></p><p>Step3: Obtain enhanced data sets Remember to change the number ofGpus before you start --nproc_per_node=GPU_numbers It took 6:31min onthe two A6000s <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./train/run.sh<br></code></pre></td></tr></table></figure></p><p>Inference: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./inference/instruction_filter.sh<br></code></pre></td></tr></table></figure></p><p>Necessity evaluation: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs script">python  ./necessity-evaluation/necessity-evaluation.py ./result.json ./inference-necessity-evaluation.json <br>python ./necessity-evaluation/necessity-instruction-extraction.py ./inference-necessity-evaluation.json ./<br>necessity-instruction.json 0<br></code></pre></td></tr></table></figure></p><p>all number of instructions 100</p><p>The percent of each score: (4, 5) : 1 0.01 (-5, -4) : 3 0.03 (5, 6) :8 0.08 (-2, -1) : 16 0.16 (-3, -2) : 6 0.06 (2, 3) : 10 0.1 (7, 8) : 90.09 (6, 7) : 3 0.03 (-1, 0) : 9 0.09 (1, 2) : 10 0.1 (-7, -6) : 1 0.01(0, 1) : 13 0.13 (8, 9) : 2 0.02 (3, 4) : 5 0.05 (-6, -5) : 2 0.02 (-4,-3) : 2 0.02 num of bad case : 39</p><p>Merge data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./necessity-evaluation/merge.py ./seed-instructions.json ./necessity-instruction.json  ./final-selected-dataset.json<br></code></pre></td></tr></table></figure> number of file 1 500</p><p>number of file 2 39</p><p>number of data 539</p><p>Step 4:Train LLM Fine-tuning the raw LLM with the merged datasetagain: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./train/run.sh<br></code></pre></td></tr></table></figure></p><h3id="self-evolved-diverse-data-sampling-for-efficient-instruction-tuning">Self-EvolvedDiverse Data Sampling for Efficient Instruction Tuning</h3><p>One sentence summary: Use a self-iterative approach to get the mostdifferentiated training data</p><p>Evaluation: Is K-Center-Sampling too computation-intensive for largetraining sets? And the evaluation structure also relies on GPT-4judge.</p><h5 id="design-experiment-5">Design experiment</h5><img src="/2024/10/21/Implementation/16.png" class=""><p>There is the initial training pool <spanclass="math inline">\(P_0\)</span>, and the training pool to be selectedis <span class="math inline">\(Q_0\)</span></p><ol type="1"><li><p>In <span class="math inline">\(P_0\)</span> training model <spanclass="math inline">\(M_0\)</span>, then embedding <spanclass="math inline">\(Q_0\)</span> and <spanclass="math inline">\(P_0\)</span></p></li><li><p>K-center-sampling algorithm is used to calculate the K pointsfarthest from <span class="math inline">\(P_0\)</span> in <spanclass="math inline">\(Q_0\)</span> and select them as <spanclass="math inline">\(S_0\)</span> and add them into <spanclass="math inline">\(P_0\)</span> for the next round of trainingdata</p></li><li><p>Iterate</p></li></ol><h5 id="reproduce-process-3">Reproduce process</h5><p>First use the debug file to see if it works: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">CUDA_VISIBLE_DEVICES=0,1 nohup python -m torch.distributed.run --nproc_per_node=2 train.py --config_file configs/config_debug.yml --wandb_key 96a8969282485b1be0c7897993485cc613bf59aa &gt; train_log.out 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure></p><p>Once clear, start sifting through the data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">CUDA_VISIBLE_DEVICES=0,1 nohup python -m torch.distributed.run --nproc_per_node=2 train.py --config_file configs/config_example_kc_dolly.yml --wandb_key 96a8969282485b1be0c7897993485cc613bf59aa &gt; train_log.out 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure></p><p>For some reason, the author seems to use full fine-tuning, I have noenough resources so can not finetuning...</p>]]></content>
    
    
    <categories>
      
      <category>PROJECT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PAPER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Project</title>
    <link href="/2024/10/21/Project/"/>
    <url>/2024/10/21/Project/</url>
    
    <content type="html"><![CDATA[<p>This post is a general description of my resume, especially myproject experience. My complete CV can be found at: <ahref="https://github.com/radicalyyyahaha/Curriculum-Vitae/blob/main/Zhili%20Yang.pdf">CV</a></p><h1 id="education">Education</h1><img src="/2024/10/21/Project/1.png" class=""><h1 id="research-experience">Research Experience</h1><h3 id="data-selection">Data Selection</h3><p>See details in post: <ahref="/2024/10/21/Implementation">Implementation of Data FilteringPapers</a></p><h3 id="nus-summer-workshop">NUS Summer Workshop</h3><ul><li>Final grade shows here: <ahref="https://github.com/radicalyyyahaha/NUS-SOC/blob/main/Final%20grade.pdf">Feedback</a></li><li>Github repo: <ahref="https://github.com/radicalyyyahaha/NUS-SOC">2024 NUS</a></li><li>Poster for pre here: <ahref="https://github.com/radicalyyyahaha/NUS-SOC/blob/main/NUS%20final.pdf">POSTER</a></li></ul><p>The program began in May with more than a month of classes followedby several exams. After that, I successfully entered the <strong>deeplearning &amp; robotics</strong> cluster through double selection. Thefinal project is to design and choose according to the single chipmicrocomputer. I am the leader of our group, and what our group decidedto do is a <strong>cyberpetpet</strong>. The part I was responsible forwas the algorithm programming, which was mainly to train an algorithmthat could <strong>track a specific object</strong>, and then after thecyberpet caught the object, it could have the return and only return theobject to the owner, and it also came with a <strong>basic voice commandfollowing</strong> system, such as dancing in circles, performing tasks,etc.</p><p>The specific method is to train some object detection algorithms,such as YOLO, DETR (the final decision is YOLO). In order to solve theproblem of inaccurate final detection due to too few training classes,we have made many attempts, such as modifying the model structure andincreasing the data sample size. As for the voice instruction followmodule, we use the existing open source speech2text, and then convertthe voice instructions into digital signals to the MCU forexecution.</p><h3 id="d-panoramic-simulation-for-metaverse">3D Panoramic Simulationfor Metaverse</h3><ul><li>I am the person in charge of this project, you can find the detailsof the project description here: <ahref="https://github.com/radicalyyyahaha/Curriculum-Vitae/blob/main/files/%E3%80%90%E7%A7%91%E6%8A%80%E5%8F%91%E6%98%8E%E5%88%B6%E4%BD%9C%E7%B1%BB%E3%80%91%E5%B9%BF%E4%B8%9C%E7%9C%81%E7%A7%91%E6%8A%80%E5%88%9B%E6%96%B0%E6%88%98%E7%95%A5%E4%B8%93%E9%A1%B9%E8%B5%84%E9%87%91%E7%94%B3%E6%8A%A5%E4%B9%A6.pdf">projectproposal in Chinese</a></li><li>A patent is pending</li></ul><p>The aim of this project is to design a visual product for<strong>real-time acquisition and transmission of three-dimensionalpanoramas</strong> in the meta-universe. By combining the improved 2DLiDAR with fisheye lens, the acquisition and data fusion of low costpanoramic video with field depth are realized, and then transmitted andpresented to the client.</p><p>The main research contents are as follows: 1. The two-dimensionallaser ranging device is improved so that it can carry out high-frequencythree-dimensional ranging in a small range and obtain three-dimensionalpoint cloud data to meet the real-time tracking requirements; 2. 2.Through the fisheye camera imaging technology, the 360-degree real sceneimage is obtained, and then the steps such as stitching, correction,coding and transmission are carried out to basically realize thegeneration of real-time images; 3. The panoramic real scene imageobtained by the fisheye lens is accurately matched with thethree-dimensional point cloud data obtained by the liDAR to generate thebasic three-dimensional spatial information with the image distance,basically meeting the modeling requirements.</p><p><strong>My job is led project scheduling and contributed to the fishcamera splicing algorithm for a 3D panorama acquisition andtransmission. Designed and modeled the product's appearance aswell.</strong> Key issues to be addressed: The algorithm design ofbinocular fisheye lens wide-angle video efficient synthesis panoramicvideo, using two 180 degree fisheye lens synthesis 360 degree panorama,effectively reduce the amount of visual perception data. A user-friendlypresentation based on VR devices; Adopt deep learning to achieveaccurate and efficient matching technology between panoramic video dataand three-dimensional space point cloud.</p><h1 id="course-project">Course Project</h1><h3 id="java-project-2022-spring">Java project (2022 Spring)</h3><img src="/2024/10/21/Project/2.jpg" class=""><p>In the final project of my freshman Java course, I built a chess gamefrom scratch and then wrote a very simple AI program for it: <strong>thebot would make random choices</strong> based on every move you madewithin a reasonable range.</p><h3 id="integrated-design-2022-fall">Integrated Design (2022 Fall)</h3><img src="/2024/10/21/Project/4.jpg" class=""><p>In my previous major, the final project was to design a smart trashcan, for which I wrote <strong>a simple face recognitionsystem</strong>, that is, when people come, they will open the lid ofthe trash can, and when people walk back, they will close it. It was avery simple task, but it was also my first experience with AIalgorithms, after which I switched to data science.</p><h3 id="heuristic-search-2023-spring">Heuristic Search (2023Spring)</h3><img src="/2024/10/21/Project/3.jpg" class=""><p>In the course of Data structure and algorithm analysis, the finalproject is to <strong>expand the <spanclass="math inline">\(ARA^*\)</span> algorithm on the basis of A*algorithm</strong> to complete the path planning problem <strong>fromsub-optimal to optimal in a maze that changes at any time</strong>. Thisproject was also the beginning of my introduction to reinforcementlearning.</p><h3 id="sound-data-classification-2023-fall">Sound Data Classification(2023 Fall)</h3><ul><li>Github repo: <ahref="https://github.com/radicalyyyahaha/ESC-50-classification">ESC-50classification</a></li></ul><p>In the final project of the statistical learning, three other teammembers and I used different algorithms to classify ESC-50 datarespectively. My approach is to <strong>convert the sound data intoMayer spectrogram as model input, and then design a convolutional neuralnetwork model based on residual network for training.</strong></p><h3 id="improved-yolo-v8-2023-fall">Improved YOLO v8 (2023 Fall)</h3><ul><li>Github repo: <ahref="https://github.com/radicalyyyahaha/yolov8">code andreport</a></li></ul><p>In the final project of artificial intelligence offered by ourschool, our group <strong>made progress in the task of capturing tinyobjects by adding a small attention module to yolo v8n.</strong> Thefinal result was that the value of the trained model mAP50-95 reached0.35, which was 0.02 lower than that of yolo v8n, presumably because ofinsufficient training time (we only trained 25h on a 32G v100).</p><h3 id="berkeley-cs188-2024-spring">Berkeley CS188 (2024 Spring)</h3><p>You can find all projects in Github: <ahref="https://github.com/radicalyyyahaha/Berkeley-CS188">CS-188</a></p><h1 id="some-other-explorations">Some other explorations:</h1><p>Here are some other small projects I've done, even some of them I'mnot able to find a repo anymore. (So sad lost them)</p><h3 id="poem-generator">Poem Generator</h3><ul><li>Github repo: <ahref="https://github.com/radicalyyyahaha/Poem_Generator">PoemGenerator</a></li></ul><img src="/2024/10/21/Project/6.jpg" class=""><p>Modeled following nanoGPT, a decoder-only transformer model isconstructed from scratch, and then it is used for poetry training andgeneration after a huge modification.</p><h3id="protein-morphology-prediction-method-based-on-deep-learning">Proteinmorphology prediction method based on Deep Learning</h3><p>In this research, I mainly focus on and learn <strong>how to use deeplearning models for protein sequence and structure prediction</strong>,especially application language models and Transformer architecture.I've been <strong>looking at Ali Madani's ProGen model</strong>, whichis based on a variant of Transformer for accurate prediction of proteinsequences, providing fine control in terms of primary sequencesimilarity, secondary structure accuracy, and conformational energy.</p><p>In addition, I explored <strong>the esm-2 model from the Facebook AIteam</strong> and learned that the model simplifies the architecture bypaying attention to MSA rows and columns, which not only enablesend-to-end protein spatial structure prediction, but also has fasttraining and reasoning speed.</p><p>The project only went through a month of research, after which noprogress was made.</p><h3 id="qq-chatbot">QQ chatbot</h3><ul><li>The official website of the architecture platform: <ahref="https://nonebot.dev/">NONEBOT</a> <img src="/2024/10/21/Project/5.jpg" class=""></li></ul><p>Out of interest, I rented a 4C8G server in Tencent Cloud for half ayear for the development of QQ robots. The architecture used is NoneBot,which is a <strong>Python asynchronous QQ robot framework,</strong> itwill parse and process the message received by the QQ robot, anddistribute it to the corresponding command processor and naturallanguage processor <strong>in the form of plug-ins to complete specificfunctions.</strong> NoneBot uses the aiocqhttp (opens new window)library for its underlying interaction, which is a Python asynchronousSDK for OneBot (opens new window) (formerly CQHTTP) that usesasynchronous I/O, Encapsulates Web server related code and supportsOneBot's HTTP, HTTP POST, and reverse WebSocket communication methods.NoneBot cannot run without the OneBot implementation, taking go-cqhttpas an example, which communicates directly with the QQ server for theactual receiving and sending of messages, notifications, and requests.When go-cqhttp receives the message, it wraps the message as an event(the same as a notification and request) and sends the event to theNoneBot according to the universal address in ws-reverse in itsconfiguration.</p><p>Less than two months after developing the functions of the QQ bot, alarge number of plug-ins come from the plug-in market, and <strong>whatI do is more the task of integrating plug-ins</strong>(thanks to theplugin authors for their selfless contributions). Sadly, Tencentofficials began to ban the QQ bot on a large scale, and with the serverthat I rented for a few days, and then I shut down the server.</p><h3 id="stable-diffusion-so-vits-svc">Stable Diffusion &amp;So-vits-SVC</h3><ul><li>Trained ckpt stored in huggingface:<ahref="https://huggingface.co/Misukumika/so-vit-svc/tree/main/so-vits%20models">so-vitsmodels</a></li></ul><p>Make easy AI cover videos with stable diffusion and So-vits-SVC, seemy bilibili channel for details: <ahref="https://space.bilibili.com/364701513">bilibili</a></p>]]></content>
    
    
    <categories>
      
      <category>PROJECT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Research Experience, Course Project</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Supervised Contrastive Learning</title>
    <link href="/2024/06/20/Supervised/"/>
    <url>/2024/06/20/Supervised/</url>
    
    <content type="html"><![CDATA[<p>This paper can be considered an improvement over self-supervisedcontrastive learning, offering a method that helps enhance the qualityof features.</p><h3id="introduction-to-self-supervised-contrastive-learning">Introductionto Self-Supervised Contrastive Learning</h3><img src="/2024/06/20/Supervised/1.png" class=""><p>Self-supervised learning involves pre-training a model using a largeamount of unlabeled data to learn data representations. These learnedrepresentations are then used in various downstream tasks, where theyare connected to different classifiers/regressors and trained usinglabeled data from the downstream tasks, making them suitable for thosetasks.</p><p>Essentially, this maps the data onto a high-dimensional hypersphere,where vectors with similar features have high cosine similarity andvectors with dissimilar features are far apart. Word embedding seems toutilize this concept.</p><p>Different models define their losses slightly differently. Here arethe two most classic models:</p><h5 id="simclr">SimCLR</h5><p><span class="math display">\[ L = -\log \frac{\exp(\text{sim}(h_i,h_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \ne i]} \exp(\text{sim}(h_i,h_k)/\tau)} \]</span></p><h5 id="moco">MoCo</h5><p>Momentum Contrast (MoCo) uses a dynamically updated queue to storenegative samples. Its loss function is similar to SimCLR but employs amomentum encoder to generate contrastive representations. <spanclass="math display">\[ L = -\log \frac{\exp(\text{sim}(h_q,h_k^+)/\tau)}{\sum_{i=0}^{K} \exp(\text{sim}(h_q, h_{k_i})/\tau)}\]</span> Where: - $ h_q $ is the representation of the query sample. -$ h_k^+ $ is the representation of the positive sample. - $ h_{k_i} $ isthe representation of the negative samples in the queue.</p><p>While this method can learn good features, it has a shortcoming: itdoes not consider the correlation of features among different imagesbelonging to the same class. <img src="/2024/06/20/Supervised/2.png" class=""></p><h3 id="supervised-contrastive-learning">Supervised ContrastiveLearning</h3><p>To make the features of similar images close to each other, classinformation is used to determine which images belong to the same class.Therefore, the method's name changes from "self-supervised" to"supervised". The basis for contrastive learning shifts from "whetherthey come from the same image" to "whether they belong to the sameclass". The loss function used in training becomes:</p><img src="/2024/06/20/Supervised/3.png" class=""><p>The authors designed two types of loss functions, one with theweighting sum outside the log and one with it inside. Experiments foundthat the first type works better.</p>]]></content>
    
    
    <categories>
      
      <category>PAPER</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SCL, SSCL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kullback-Leibler Divergence</title>
    <link href="/2024/06/20/Divergence/"/>
    <url>/2024/06/20/Divergence/</url>
    
    <content type="html"><![CDATA[<p>Before introducing KL divergence, we need to understand CE lossfirst.</p><h3 id="cross-entropy">Cross-entropy</h3><p>Why not MSE: In statistical learning, when we need to predict aspecific value, such as in a regression model, MSE can be used to definethe loss. However, if the final prediction is to select one frommultiple classes, i.e., the prediction results will be probabilitiesdistributed across different classes, we need to use cross-entropyloss.</p><p>For the general definition of entropy, we usually consider:</p><p><span class="math display">\[H=-\sum P(X)\log P(X)\]</span></p><p>Where <span class="math inline">\(P(x)\)</span> is the probability(frequency) of the event occurring, <span class="math inline">\(-\logP(x_i) = \log \frac{1}{P(x_i)}\)</span>, can be understood as the numberof occurrences of the event within a unit statistical count. Thisformula well demonstrates the nature of entropy: when an event is verycertain (probability close to 0 or 1), its entropy is close to 0;conversely, if we are unsure, such as flipping a fair coin, the entropyis larger.</p><p>As for cross-entropy, as the name suggests, when the true probabilitydistribution of an event is <span class="math inline">\(P(X)\)</span>,and we use an approximate distribution (Q(X)) to fit it, the entropy atthis time should be:</p><p><span class="math display">\[H(P, Q)=-\sum P(X)\log Q(X)\]</span></p><p>In the cross-entropy loss function, <spanclass="math inline">\(P(X)\)</span> generally contains the one-hotencoding of the true labels. For example, for binary classificationproblems, the well-known BCELoss() should be:</p><p><spanclass="math display">\[BCE=-y\log(\hat{y})-(1-y)\log(1-\hat{y})\]</span></p><h3 id="kl-divergence">KL Divergence</h3><p>KL divergence is used to measure the difference between probabilitydistributions. More specifically, it describes the amount of informationloss when we use <span class="math inline">\(Q(X)\)</span> to fit <spanclass="math inline">\(P(X)\)</span>:</p><p><span class="math display">\[D_{KL}(P||Q)=H(P,Q)-H(P)=\sumP(X)\log\frac{P(X)}{Q(X)}\]</span></p><p>Clearly, KL divergence is not symmetric, and therefore it is not ametric.</p><p>Noting that if <span class="math inline">\(g\)</span> is anyreal-valued measurable function, and <spanclass="math inline">\(\varphi\)</span> is a convex function within therange of <span class="math inline">\(g\)</span>, then</p><figure><imgsrc="https://wikimedia.org/api/rest_v1/media/math/render/svg/d33a0b42492bc7a9c197ec7c8717f5a073236f5e"alt="{({-}^{}g(x)f(x),dx){-}^{}(g(x))f(x),dx.}" /><figcaptionaria-hidden="true">{(<em>{-}^{}g(x)f(x),dx)</em>{-}^{}(g(x))f(x),dx.}</figcaption></figure><p>Using this property, it is not difficult to find:</p><p><span class="math display">\[D_{KL}(P||Q)=\intP(X)\log\frac{P(X)}{Q(X)} \ge -\log\int Q(X) =0 \]</span></p><p>Example: Calculating KL divergence between two multivariate Gaussiandistributions: <img src="/2024/06/20/Divergence/1.jpg" class=""></p>]]></content>
    
    
    <categories>
      
      <category>MATH</category>
      
    </categories>
    
    
    <tags>
      
      <tag>KL, CE</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

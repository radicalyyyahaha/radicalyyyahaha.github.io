

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/head.png">
  <link rel="icon" href="/img/head.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhili Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="Overview In the early research, the work of instruction tuning is mainly focused on the construction of large-scale instruction data sets, and there are two main ways to create instruction data sets.">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementation">
<meta property="og:url" content="http://example.com/2024/10/21/Implementation/index.html">
<meta property="og:site_name" content="Zhili Yang&#39;s Blog">
<meta property="og:description" content="Overview In the early research, the work of instruction tuning is mainly focused on the construction of large-scale instruction data sets, and there are two main ways to create instruction data sets.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/impl.png">
<meta property="article:published_time" content="2024-10-21T12:03:13.000Z">
<meta property="article:modified_time" content="2024-10-30T14:44:18.756Z">
<meta property="article:author" content="Zhili Yang">
<meta property="article:tag" content="PAPER">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/impl.png">
  
  
  
  <title>Implementation - Zhili Yang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":55,"cursorChar":">_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"V00BmRbjy0xa9r1TkJZRqVX4-MdYXbMMI","app_key":"Oht7XhOFVBgTl4ILm6USAWE9","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Zhili Yang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/impl.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Implementation"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-10-21 20:03" pubdate>
          October 21, 2024 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.4k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          21 mins
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Implementation</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="overview">Overview</h1>
<p>In the early research, the work of instruction tuning is mainly
focused on the construction of large-scale instruction data sets, and
there are two main ways to create instruction data sets. One is to
convert a text-label pair from an existing annotated natural language
dataset to an instruction-output pair, such as P3, via a template.
Another way is to use an LLM such as GPT-3.5-Turbo to generate output
for a given instruction. While instruction fine-tuning relies primarily
on large amounts of data, LIMA and others have shown that data quality
is more critical than quantity. They demonstrated a significant increase
in LLM performance using just 1k high-quality instruction data. This
finding suggests that LLM has acquired world knowledge in the
pre-training phase and requires only a small amount of high-quality
instruction data to generate high-quality responses in the instruction
tuning phase.</p>
<p>Various instruction tuning datasets generated by LLMS, such as
Self-Instruct and Alpaca, provide large samples without manual
manipulation, but their data quality depends on the LLM's performance
and is uncertain. In contrast, hand-collated datasets, such as LIMA and
Dolly, achieve higher quality through human selection, but may be
subject to human bias. Alternative data set construction methods, such
as prompt mapping and mapping, aim to improve data set quality and
diversity, but present new challenges in terms of quality assurance.
This variability in data set construction and source can significantly
affect data quality, highlighting the importance of careful data
selection for LLM instruction tuning.</p>
<p>The methods can be divided into the following four categories:
methods based on system of indicators, trainable LLMs, powerful LLMs
like ChatGPT and small models.</p>
<p>The effectiveness of a data selection method depends on the quality
of the selected subset from a given data set. In order to measure the
quality of subsets, LLMS fine-tuned on subsets are evaluated on
different benchmarks by different methods, which can be divided into
three categories: wining rate, inner comparison and external
comparison.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.05123">A Survey on Data
Selection for LLM Instruction Tuning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.16827">A Survey on Data
Selection for Language Models</a></li>
</ul>
<h3
id="from-quantity-to-quality-boosting-llm-performance-with-self-guided-data-selection-for-instruction-tuning">From
Quantity to Quality: Boosting LLM Performance with Self-Guided Data
Selection for Instruction Tuning</h3>
<p>One sentence summary: Let the model choose the data for itself</p>
<h5 id="design-experiment">Design experiment</h5>
<ol type="1">
<li>Use k-means to filter out 1k instructions to fine-tune the data for
training and train only one epoch</li>
<li>This cherry-model is then used to filter the data in reverse, using
the IFD metric</li>
<li><span class="math display">\[IFD_\theta(Q,
A)=\frac{s_\theta(A|Q)}{s_\theta(A)}\]</span></li>
</ol>
<img src="/2024/10/21/Implementation/1.png" srcset="/img/loading.gif" lazyload class="">
<ol start="4" type="1">
<li>This formula selects data with IFD less than 1 and a high ratio,
which means that these data are difficult for the model to learn</li>
<li>Then use this part to filter out more valuable training data to
train the model</li>
</ol>
<h5 id="reproduce-process">Reproduce process</h5>
<p>Use configuration: <img src="/2024/10/21/Implementation/2.png" srcset="/img/loading.gif" lazyload class=""> 1. Select 1k pieces of data
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_pre.pt \<br>    --model_name_or_path &quot;meta-llama/Llama-3.1-8B&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod pre<br></code></pre></td></tr></table></figure></p>
<p>--data_path: The targeted dataset in the Alpaca format</p>
<p>--save_path: The path to save the .pt file containing embeddings or
scores</p>
<p>--prompt: The prompt type used for training and selecting data, can
choose between alpaca or wiz</p>
<p>Change in <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">        <span class="hljs-keyword">elif</span> args.prompt == <span class="hljs-string">&#x27;code&#x27;</span>:<br>            input_i = data_i[<span class="hljs-string">&#x27;prompt&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;prompt&#x27;</span> <span class="hljs-keyword">in</span> data_i.keys() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span><br>            <span class="hljs-keyword">if</span> input_i == <span class="hljs-string">&#x27;&#x27;</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_no_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>            <span class="hljs-keyword">else</span>:<br>                temp_dict = &#123;<span class="hljs-string">&#x27;instruction&#x27;</span>:instruct_i,<span class="hljs-string">&#x27;prompt&#x27;</span>:input_i&#125;<br>                promt_to_use = PROMPT_DICT[<span class="hljs-string">&quot;prompt_input&quot;</span>].format_map(temp_dict)<br>                whole_text = promt_to_use + output_i<br>                instruct_i = promt_to_use<br>                <br>           <br><span class="hljs-keyword">and</span> some other replacement of <span class="hljs-built_in">input</span>-prompt, output-response<br></code></pre></td></tr></table></figure></p>
<p>--mod: pre used for getting needed embeddings or scores on selecting
pre-experienced samples and cherry used for cherry</p>
<img src="/2024/10/21/Implementation/3.png" srcset="/img/loading.gif" lazyload class="">
<p>Some parameters are on the meta device because they were offloaded to
the cpu. Reason: Probably because of insufficient GPU memory</p>
<p>It was estimated to be about 25min, but it was actually very, very
long... But The author also admitted it in limitation. the main
limitation of this method is the inconvenience of training the
pre-experienced model.</p>
<p>So I randomly selected 1k pieces of data to feel ~ Time Used:
27.98344091176987 (min)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_cluster.py \<br>    --pt_data_path code_data_pre.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_pre.json \<br>    --sample_num 8 \<br>    --kmeans_num_clusters 100 \<br>    --low_th 25 \<br>    --up_th 75<br></code></pre></td></tr></table></figure>
<p>--pt_data_path: The .pt file from previous step containing needed
embeddings or scores</p>
<p>--json_data_path: The targeted dataset in the Alpaca format</p>
<p>--json_save_path: The path to save the selected pre-experienced
samples</p>
<p>--sample_num: How many samples will be selected in each cluster</p>
<p>--kmeans_num_clusters: How many clusters will be generated by
K-Means</p>
<p>--low_th and --up_th: The lower and Upper threshold for selecting
samples within each cluster</p>
<p>In an instant, 420 items were selected (why 420 items? Is it because
each cluster is not selected?)</p>
<ol start="2" type="1">
<li>Train Pre-Experienced Model <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">!pip install unsloth<br></code></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> FastLanguageModel<br><span class="hljs-keyword">import</span> torch<br>max_seq_length = <span class="hljs-number">2048</span> <span class="hljs-comment"># Choose any! We auto support RoPE Scaling internally!</span><br>dtype = <span class="hljs-literal">None</span> <span class="hljs-comment"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span><br>load_in_4bit = <span class="hljs-literal">True</span> <span class="hljs-comment"># Use 4bit quantization to reduce memory usage. Can be False.</span><br><br><br>model, tokenizer = FastLanguageModel.from_pretrained(<br>    model_name = <span class="hljs-string">&quot;unsloth/Meta-Llama-3.1-8B&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dtype = dtype,<br>    load_in_4bit = load_in_4bit,<br>    <span class="hljs-comment"># token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span><br>)<br><br><br>model = FastLanguageModel.get_peft_model(<br>    model,<br>    r = <span class="hljs-number">16</span>, <span class="hljs-comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span><br>    target_modules = [<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>,<br>                      <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>,],<br>    lora_alpha = <span class="hljs-number">16</span>,<br>    lora_dropout = <span class="hljs-number">0</span>, <span class="hljs-comment"># Supports any, but = 0 is optimized</span><br>    bias = <span class="hljs-string">&quot;none&quot;</span>,    <span class="hljs-comment"># Supports any, but = &quot;none&quot; is optimized</span><br>    <span class="hljs-comment"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span><br>    use_gradient_checkpointing = <span class="hljs-string">&quot;unsloth&quot;</span>, <span class="hljs-comment"># True or &quot;unsloth&quot; for very long context</span><br>    random_state = <span class="hljs-number">3407</span>,<br>    use_rslora = <span class="hljs-literal">False</span>,  <span class="hljs-comment"># We support rank stabilized LoRA</span><br>    loftq_config = <span class="hljs-literal">None</span>, <span class="hljs-comment"># And LoftQ</span><br>)<br><br><br>alpaca_prompt = <span class="hljs-string">&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Instruction:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Input:</span><br><span class="hljs-string">&#123;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">### Response:</span><br><span class="hljs-string">&#123;&#125;&quot;&quot;&quot;</span><br><br>EOS_TOKEN = tokenizer.eos_token <span class="hljs-comment"># Must add EOS_TOKEN</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">formatting_prompts_func</span>(<span class="hljs-params">examples</span>):<br>    instructions = examples[<span class="hljs-string">&quot;instruction&quot;</span>]<br>    inputs       = examples[<span class="hljs-string">&quot;prompt&quot;</span>]<br>    outputs      = examples[<span class="hljs-string">&quot;response&quot;</span>]<br>    texts = []<br>    <span class="hljs-keyword">for</span> instruction, <span class="hljs-built_in">input</span>, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(instructions, inputs, outputs):<br>        <span class="hljs-comment"># Must add EOS_TOKEN, otherwise your generation will go on forever!</span><br>        text = alpaca_prompt.<span class="hljs-built_in">format</span>(instruction, <span class="hljs-built_in">input</span>, output) + EOS_TOKEN<br>        texts.append(text)<br>    <span class="hljs-keyword">return</span> &#123; <span class="hljs-string">&quot;text&quot;</span> : texts, &#125;<br><span class="hljs-keyword">pass</span><br><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;code_data_pre.json&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)<br>dataset = dataset.<span class="hljs-built_in">map</span>(formatting_prompts_func, batched = <span class="hljs-literal">True</span>,)<br><br><br><br><span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTTrainer<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> is_bfloat16_supported<br><br>trainer = SFTTrainer(<br>    model = model,<br>    tokenizer = tokenizer,<br>    train_dataset = dataset,<br>    dataset_text_field = <span class="hljs-string">&quot;text&quot;</span>,<br>    max_seq_length = max_seq_length,<br>    dataset_num_proc = <span class="hljs-number">2</span>,<br>    packing = <span class="hljs-literal">False</span>, <span class="hljs-comment"># Can make training 5x faster for short sequences.</span><br>    args = TrainingArguments(<br>        per_device_train_batch_size = <span class="hljs-number">2</span>,<br>        gradient_accumulation_steps = <span class="hljs-number">4</span>,<br>        warmup_steps = <span class="hljs-number">5</span>,<br>        num_train_epochs = <span class="hljs-number">1</span>, <span class="hljs-comment"># Set this for 1 full training run.</span><br>        <span class="hljs-comment">#max_steps = 60,</span><br>        learning_rate = <span class="hljs-number">2e-4</span>,<br>        fp16 = <span class="hljs-keyword">not</span> is_bfloat16_supported(),<br>        bf16 = is_bfloat16_supported(),<br>        logging_steps = <span class="hljs-number">1</span>,<br>        optim = <span class="hljs-string">&quot;adamw_8bit&quot;</span>,<br>        weight_decay = <span class="hljs-number">0.01</span>,<br>        lr_scheduler_type = <span class="hljs-string">&quot;linear&quot;</span>,<br>        seed = <span class="hljs-number">3407</span>,<br>        output_dir = <span class="hljs-string">&quot;outputs&quot;</span>,<br>    ),<br>)<br><br>trainer.train()<br></code></pre></td></tr></table></figure>
<p>Time Used: 8:03min</p>
<ol start="3" type="1">
<li>通过IFD选cherry data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_analysis.py \<br>    --data_path data/code.json \<br>    --save_path code_data_cherry.pt \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 512 \<br>    --prompt code \<br>    --mod cherry<br></code></pre></td></tr></table></figure> Time Used: 6.185897000630697
(min)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">python cherry_seletion/data_by_IFD.py \<br>    --pt_data_path code_data_cherry.pt \<br>    --json_data_path data/code.json \<br>    --json_save_path code_data_cherry.json \<br>    --model_name_or_path &quot;outputs/checkpoint-52&quot; \<br>    --max_length 2048 \<br>    --sample_rate 0.06 \<br>    --prompt code<br></code></pre></td></tr></table></figure>
<p>--sample_rate: How many cherry samples you would like to select? You
can also use --sample_number to set the exact number of samples.
Finally, 60 pieces of data with IFD&lt;1 were obtained, and soted sorted
them from smallest to largest</p>
<p>Smallest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` Your code should pass the following test case: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) assert elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Construct a Python function `create_folds(data, count)` to create a list of folds from the given data, where each fold is a subsequence of the original data. The length of each fold should be approximately equal with at most a difference of 1. The function should satisfy the following assertion: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ### Response [Reasoning] To create a Python function that generates a list of folds from given data with each fold having approximately equal length (with at most a difference of 1 between any two folds), you can follow these steps: 1. Return an empty list immediately if `count` is non-positive since it&#x27;s not possible to create a valid number of folds. 2. Divide the total length of the data by `count`, rounding down, to get the minimum number of items each fold should have. 3. Calculate the remainder to see how many extra items need to be distributed among the folds. They are distributed one by one to the first `remainder` folds. 4. Use a loop to create each fold, adjusting the starting and ending indices based on the calculations above to ensure each fold has the correct number of items. [Implementation] Here&#x27;s how you can implement this: ```python def create_folds(data, count): # Return an empty list if count is non-positive if count &lt;= 0: return [] data_length = len(data) fold_size = data_length // count remainder = data_length % count folds = [] start_index = 0 for i in range(count): # Add 1 element to each of the first `remainder` folds end_index = start_index + fold_size + (1 if i &lt; remainder else 0) folds.append(range(start_index, end_index)) start_index = end_index return folds ``` [Explanation] This implementation ensures that each fold has an equal length with at most a difference of 1 between any two folds. It handles edge cases, such as a non-positive `count`, by returning an empty list as specified. [Tests] You can test your code with the provided assertions to verify that the function works correctly: ```python assert create_folds(range(12), 2) == [range(0, 6), range(6, 12)] assert create_folds(range(8), 2) == [range(0, 4), range(4, 8)] assert create_folds(range(25), -5) == [] assert create_folds(range(6), 2) == [range(0, 3), range(3, 6)] ``` ## Example 2 ### Instruction Create a Python function `create_element` to create an XML-like element. It takes one required argument &#x27;name&#x27; (a string) and two optional arguments &#x27;attributes&#x27; (a dictionary of name-value pairs) and &#x27;children&#x27; (a list of elements created by this function). The function should return an object that has a &#x27;name&#x27; attribute and &#x27;attributes&#x27; and &#x27;children&#x27; properties. Here is an example: ```python elem = create_element(&#x27;root&#x27;, children=[create_element(&#x27;child1&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;foo&#x27;&#125;), create_element(&#x27;child2&#x27;, attributes=&#123;&#x27;name&#x27;: &#x27;bar&#x27;&#125;)]) elem.children[0].attributes[&#x27;name&#x27;] == &#x27;foo&#x27; ``` ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To create a Python function that creates an XML-like element with a given name, attributes, and children, you can follow these steps: 1. Define a `create_element` function that takes a required &#x27;name&#x27; argument and two optional arguments &#x27;attributes&#x27; and &#x27;children&#x27;. 2. Declare a class `Element` that holds the name, attributes, and children of the element. 3. Inside the `create_element` function, create a new `Element` object with the given &#x27;name&#x27;, &#x27;attributes&#x27;, and &#x27;children&#x27;. 4. Return the newly created `Element` object. 5. Test your function with the given assertions. Here&#x27;s how you can implement this: ```python class Element: def __init__(self, name, attributes=None, children=None): self.name = name self.attributes = attributes or &#123;&#125; self.children = children or [] def create_element(name, attributes=None, children=None): return Element(name, attributes, children) ```&quot;</span><br></code></pre></td></tr></table></figure></p>
<p>bigest ifd: <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">instruction:<span class="hljs-comment">&quot;Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. Your code should pass the following test case: ```python dictionary = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; new_dict = copy_and_delete(dictionary, &#x27;a&#x27;) assert new_dict == &#123;&#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; ```&quot;</span><br>prompt:<span class="hljs-comment">&quot;Provide the best response to a given instruction. Follow the following steps to craft your response: 1. reason about the given instruction 2. provide a high-quality solution 3. offer a concise explanation 4. write tests to verify the correctness your solution ## Example 1 ### Instruction Design a Python function to encode a list of strings into a unique list with the same length, order, and meaning. The new list is formed by appending asterisks (*) to duplicate strings. For instance, given [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;], the function should return [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;], while for [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] it should return [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] without any changes. Make a precondition check through `assert` to ensure the input is a list of strings. Solve the problem in two lines of code, one for the precondition check and the other for the main logic. ### Response [Reasoning] You can achieve this functionality by first ensuring the input meets your requirements using an `assert` statement with list comprehension. Then, use another list comprehension to iterate through the input list and append `&#x27;*&#x27;` to the strings. The number of `&#x27;*&#x27;` is the number of occurrences of the string before the current index. [Implementation] Here&#x27;s how you can do it in two lines of Python code: ```python def encode_strings(input_list): assert all(isinstance(item, str) for item in input_list), &quot;</span><span class="hljs-type">Input</span> must be a list of strings<span class="hljs-comment">&quot; return [s + &#x27;*&#x27; * input_list[:i].count(s) for i, s in enumerate(input_list)] ``` [Explanation] The `enumerate` function provides both the index and the string in each iteration, allowing you to count the occurrences of the string before the current index. [Tests] Now you can test your function with the provided examples: ```python assert encode_strings([&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;a*&#x27;, &#x27;b&#x27;, &#x27;a**&#x27;, &#x27;c&#x27;] assert encode_strings([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]) == [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] ``` Note that although the function is concise, it is not the most efficient solution for large lists, as the `count` method has a time complexity of O(n) for each iteration. For larger lists, a more efficient approach might be to use a dictionary to store the counts of each string and then iterate through the list to append the appropriate number of `&#x27;*&#x27;` to each string. ## Example 2 ### Instruction Write a Python function that takes a dictionary as an argument and makes a copy of it by creating a new dictionary. Modify the copy by deleting a specific key from the dictionary if it exists. ### Response&quot;</span><br>response:<span class="hljs-comment">&quot;To make a copy of a dictionary, you can use the built-in `dict()` function, which creates a new dictionary from an existing dictionary. To delete a specific key from the dictionary, you can use the `del` keyword or the `pop()` method. Here&#x27;s how you can implement this in Python: ```python def copy_and_delete(dictionary, key): copy_dict = dict(dictionary) if key in copy_dict: del copy_dict[key] return copy_dict ``` The above code makes a copy of the dictionary using `dict()` to create a new dictionary from the existing one. Then, if the specified key exists in the copy, it is deleted from the copy using `del`. The function returns the modified copy.&quot;</span><br></code></pre></td></tr></table></figure></p>
<p>Difference: Larger ifd instructions are shorter and more abstract,
and therefore more difficult to learn?</p>
<h3
id="superfiltering-weak-to-strong-data-filtering-for-fast-instruction-tuning">Superfiltering:
Weak-to-Strong Data Filtering for Fast Instruction-Tuning</h3>
<p>One sentence summary: Demonstrate whether it is feasible to filter
fine-tuning data based on small models and then use it for instruction
fine-tuning training of large models</p>
<h5 id="evaluation">Evaluation</h5>
<p>The main reason why this article and the previous work can work is to
ensure the consistency of knowledge before and after instruction
fine-tuning, that is, before instruction fine-tuning, the model is used
to eliminate the world knowledge that is not part of the model itself
(pre-train stage). However, this work did not guarantee that with the
passage of time and the update of later time data, the new LLM would
still have the same command difficulty perception ability as the old
model such as GPT-2, that is, the sequencing differences of IFD screened
sequences became more and more large.</p>
<h5 id="abstract-conclusion">Abstract &amp; Conclusion</h5>
<img src="/2024/10/21/Implementation/4.png" srcset="/img/loading.gif" lazyload class="">
<p>Firstly, the consistency of small model and large model perception of
instruction complexity is proved, and based on this, an instruction
fine-tuning method named "Super filtering" is proposed. Specifically,
the small model is used to evaluate the instruction data, and then the
filtered data is used to fine-tune the larger model.</p>
<p>IFD is still used as an evaluation indicator. Fine-tune the data set
for a given instruction, GPT-2 model and others directly used to
calculate the IFD score of each sample. Then the top k-percent samples
with the highest IFD scores under 1 are selected for faster instruction
tuning.</p>
<h5 id="design-experiment-1">Design experiment</h5>
<ol type="1">
<li>Dataset: Alpaca Dataset; Models: GPT-2, GPT-2-Large, GPT-2-XL,
GPT-NEO(1.3B), LlaMA3-8B</li>
<li>Calculate ppl and IFD scores of this dataset on different models and
visualize the results</li>
<li>Taking the results of LlaMA3-8b as reference, the ppl and IFD score
ranking obtained by other models were calculated with the similarity
spearman coefficient ##### Reproduce process</li>
<li>Calculate ppl and ifd scores for each model on the dataset
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/data_analysis.py \<br>    --data_path data/alpaca_data.json \<br>    --save_path alpaca_data_gpt2_scores.jsonl \<br>    --model_name_or_path gpt2 <br><br>Same with the rest<br></code></pre></td></tr></table></figure></li>
</ol>
<p>Starting with xl really takes time...</p>
<p>Due to limited video memory, a total of three Windows were opened at
the same time, and the final time took about 2.5h Because LlaMA3.1-8B is
too slow, the final result does not include it</p>
<ol start="2" type="1">
<li>Map score back to the original data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/put_analysis_to_data.py \<br>    --pt_data_path alpaca_data_gpt2_scores.jsonl \<br>    --json_data_path data/alpaca_data.json \<br>    --json_save_path 1/alpaca_data_gpt2_data.json<br><br>Same with the rest<br></code></pre></td></tr></table></figure></li>
</ol>
<p>This process is fast</p>
<ol start="3" type="1">
<li>Organize the data sequence <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">python code_ifd/select_data.py \<br>    --json_data_path 1/alpaca_data_gpt2_data.json \<br>    --json_save_path alpaca_data_gpt2_data_full.json \<br>    --sample_rate 1 \<br>    --filter_threash 2<br><br>Same with the rest<br></code></pre></td></tr></table></figure> Be careful to set threash
to prevent ifd&gt;1 from being all shaved clean</li>
</ol>
<h5 id="ppl">ppl</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span>  <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ppl_A_condition&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ppl_A_condition&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.9</span>)<br>            lower_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.01</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> lower_limit &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure>
<p>After repeated experiments, it is found that the ppl diagram in the
paper refers to ppl_A_condition, and the upper limit is 0.9 quantile</p>
<img src="/2024/10/21/Implementation/5.png" srcset="/img/loading.gif" lazyload class="">
<h5 id="idf">idf</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>folder_path = <span class="hljs-string">&#x27;data/&#x27;</span> <br><br>file_names = [<span class="hljs-string">&#x27;alpaca_data_gpt2_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-large_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;alpaca_data_gpt-neo_data_full.json&#x27;</span>]<br>file_paths = [os.path.join(folder_path, file_name) <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> file_names]<br><br>model_labels = [<span class="hljs-string">&#x27;gpt2&#x27;</span>, <span class="hljs-string">&#x27;gpt2-large&#x27;</span>, <span class="hljs-string">&#x27;gpt2-xl&#x27;</span>, <span class="hljs-string">&#x27;gpt-neo&#x27;</span>]<br><br>filtered_ifd_ppl_dict = &#123;&#125;<br><br><span class="hljs-keyword">for</span> file_path, model_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(file_paths, model_labels):<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>            alpaca_data = json.load(file)<br>            ifd_ppl_values = [item[<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> alpaca_data <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ifd_ppl&#x27;</span> <span class="hljs-keyword">in</span> item]<br>            <br>            upper_limit = np.quantile(ifd_ppl_values, <span class="hljs-number">0.99</span>)<br>            <br>            filtered_values = [value <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> ifd_ppl_values <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= value &lt;= upper_limit]<br>            <br>            filtered_ifd_ppl_dict[model_label] = filtered_values<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File not found: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error decoding JSON in file: <span class="hljs-subst">&#123;file_path&#125;</span>&quot;</span>)<br><br>plt.violinplot(dataset=[filtered_ifd_ppl_dict[model] <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> model_labels], showmedians=<span class="hljs-literal">True</span>)<br>plt.xticks(ticks=np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(model_labels) + <span class="hljs-number">1</span>), labels=model_labels, rotation=<span class="hljs-number">45</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Filtered ifd_ppl Comparison&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;ifd_ppl&#x27;</span>)<br><br>output_file = <span class="hljs-string">&#x27;./data/ifd_ppl.png&#x27;</span><br>plt.savefig(output_file)<br></code></pre></td></tr></table></figure>
<p>The upper limit of 0.99 quantile is closer to the experimental
results of the paper</p>
<img src="/2024/10/21/Implementation/6.png" srcset="/img/loading.gif" lazyload class="">
<h5 id="spearman">Spearman</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> spearmanr<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt-neo_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data1 = json.load(f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/alpaca_data_gpt2-xl_data_full.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data2 = json.load(f)<br><br>ids1 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data1]<br>ids2 = [item[<span class="hljs-string">&#x27;output&#x27;</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data2]<br><br>common_ids = <span class="hljs-built_in">set</span>(ids1) &amp; <span class="hljs-built_in">set</span>(ids2)<br><br>ranks1 = [ids1.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]  <br>ranks2 = [ids2.index(id_) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> common_ids]<br><br>spearman_corr, p_value = spearmanr(ranks1, ranks2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Spearman Rank Correlation: <span class="hljs-subst">&#123;spearman_corr&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;P-value: <span class="hljs-subst">&#123;p_value&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>neo:1 gpt2: 0.823 gpt2-large: 0.872 gpt-xl: 0.870</p>
<h3
id="less-selecting-influential-data-for-targeted-instruction-tuning">LESS:
Selecting Influential Data for Targeted Instruction Tuning</h3>
<p>One sentence summary: Perform a small amount of tuning on the
training set as warmup, then calculate the influence among all the
training data on the verification set, and finally select the data with
the largest impact factor for model training.</p>
<p>Evaluation: The computational overhead is a little too high.</p>
<h5 id="design-experiment-2">Design experiment</h5>
<img src="/2024/10/21/Implementation/9.png" srcset="/img/loading.gif" lazyload class="">
<p>Step 1: Warmup training with LoRA Efficient fine-tuning of N epoches
of FM on a random subset of a training set using LoRA yields low-rank
parameters while saving model parameters (ckpt) after each epoch.</p>
<p>Step 2: Projecting the gradients For each training data, calculate
its gradient in step 1, use RP (random projection) to generate
low-dimensional gradient features, and store these features in the
gradient datastore.</p>
<p>Step 3: Data Selection Algorithm For the val set of the target task,
for each subtask D, use the parameters <span
class="math display">\[\theta_1, \theta_2,..., \theta_N \]</span>to
calculate the average gradient feature <img src="/2024/10/21/Implementation/7.png" srcset="/img/loading.gif" lazyload class=""></p>
<p>Then the data is selected according to the similarity between the
gradient features of the data points and the verification set
<img src="/2024/10/21/Implementation/8.png" srcset="/img/loading.gif" lazyload class=""></p>
<h5 id="reproduce-process-1">Reproduce process</h5>
<p>Step 1: warm up training <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs script">DATA_DIR=./data \<br>MODEL_PATH=../hub/shakechen/Llama-2-7b-hf \<br>PERCENTAGE=0.01 \<br>DATA_SEED=3 \<br>JOB_NAME=llama2-7b-p$&#123;PERCENTAGE&#125;-lora-seed$&#123;DATA_SEED&#125; \<br><br>./less/scripts/train/warmup_lora_train.sh &quot;$DATA_DIR&quot; &quot;$MODEL_PATH&quot; &quot;$PERCENTAGE&quot; &quot;$DATA_SEED&quot; &quot;$JOB_NAME&quot;<br></code></pre></td></tr></table></figure></p>
<p>After running Step2, the optimizer.bin file is missing and only
optimizer.pt is found in the ckpt folder, check out the issues:
<img src="/2024/10/21/Implementation/10.png" srcset="/img/loading.gif" lazyload class=""></p>
<p>then run again. Cost: 3:13h on 3 A40-48G</p>
<p>Step 2: Building the gradient datastore <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs script">CKPT=12<br><br>TRAINING_DATA_NAME=dolly<br>TRAINING_DATA_FILE=./data/dolly_data.jsonl # when changing data name, change the data path accordingly<br>GRADIENT_TYPE=&quot;adam&quot;<br>MODEL_PATH=../out/llama2-7b-p0.01-lora-seed3/checkpoint-$&#123;CKPT&#125;<br>OUTPUT_PATH=../grads/llama2-7b-p0.01-lora-seed3/$&#123;TRAINING_DATA_NAME&#125;-ckpt$&#123;CKPT&#125;-$&#123;GRADIENT_TYPE&#125;<br>DIMS=&quot;8192&quot;<br><br>./less/scripts/get_info/grad/get_train_lora_grads.sh &quot;$TRAINING_DATA_FILE&quot; &quot;$MODEL_PATH&quot; &quot;$OUTPUT_PATH&quot; &quot;$DIMS&quot; &quot;$GRADIENT_TYPE&quot;<br></code></pre></td></tr></table></figure> Cost:
43:56min</p>
<p>Step 3: Selecting data for a task <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs script">CKPT=12<br>TASK=mmlu<br>MODEL_PATH=../out/llama2-7b-p0.01-lora-seed3/checkpoint-$&#123;CKPT&#125;<br>OUTPUT_PATH=../grads/llama2-7b-p0.01-lora-seed3/$&#123;TASK&#125;-ckpt$&#123;CKPT&#125;-sgd # for validation data, we always use sgd<br>DATA_DIR=../data<br>DIMS=&quot;4096 8192&quot; # We use 8192 as our default projection dimension <br><br>./less/scripts/get_info/grad/get_eval_lora_grads.sh &quot;$TASK&quot; &quot;$DATA_DIR&quot; &quot;$MODEL_PATH&quot; $OUTPUT_PATH &quot;$DIMS&quot;<br></code></pre></td></tr></table></figure></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs script">DIM=8192 # decide which dimension to use<br>GRADIENT_PATH=../grads/llama2-7b-p0.01-lora-seed3/dolly-ckpt12-adam/dim$&#123;DIM&#125;<br>TRAIN_FILE_NAMES=&quot;dolly&quot;<br>CKPTS=&quot;12&quot; # checkpoing index<br>CHECKPOINT_WEIGHTS=&quot;1.6877e-05&quot; # average lr of the epoch<br><br>VALIDATION_GRADIENT_PATH=../grads/llama2-7b-p0.01-lora-seed3/mmlu-ckpt12-sgd/dim$&#123;DIM&#125;<br>TARGET_TASK_NAMES=&quot;mmlu&quot;<br>SELECTED_DATA_OUTPUT_PATH=&quot;../selected_data&quot;<br><br>./less/scripts/data_selection/matching.sh &quot;$GRADIENT_PATH&quot; &quot;$TRAIN_FILE_NAMES&quot; &quot;$CKPTS&quot; &quot;$CHECKPOINT_WEIGHTS&quot; &quot;$VALIDATION_GRADIENT_PATH&quot; &quot;$TARGET_TASK_NAMES&quot; &quot;$SELECTED_DATA_OUTPUT_PATH&quot;<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs script">python3 -m less.data_selection.write_selected_data \<br>--target_task_names $&#123;TARGET_TASK_NAMES&#125; \<br>--train_file_names $&#123;TRAIN_FILE_NAMES&#125; \<br>--train_files ../data/train/processed/dolly/dolly_data.jsonl \<br>--output_path $SELECTED_DATA_OUTPUT_PATH \<br>--percentage 0.01<br></code></pre></td></tr></table></figure>
<p>dataset: dolly, choose top 10 data:</p>
<table>
<thead>
<tr class="header">
<th>rank</th>
<th>file name</th>
<th>index</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>dolly</td>
<td>11921</td>
<td>0.073349</td>
</tr>
<tr class="even">
<td>2</td>
<td>dolly</td>
<td>10117</td>
<td>0.072425</td>
</tr>
<tr class="odd">
<td>3</td>
<td>dolly</td>
<td>7017</td>
<td>0.071267</td>
</tr>
<tr class="even">
<td>4</td>
<td>dolly</td>
<td>14825</td>
<td>0.07023</td>
</tr>
<tr class="odd">
<td>5</td>
<td>dolly</td>
<td>4947</td>
<td>0.068469</td>
</tr>
<tr class="even">
<td>6</td>
<td>dolly</td>
<td>13839</td>
<td>0.067585</td>
</tr>
<tr class="odd">
<td>7</td>
<td>dolly</td>
<td>2031</td>
<td>0.067453</td>
</tr>
<tr class="even">
<td>8</td>
<td>dolly</td>
<td>13638</td>
<td>0.067247</td>
</tr>
<tr class="odd">
<td>9</td>
<td>dolly</td>
<td>7766</td>
<td>0.067236</td>
</tr>
<tr class="even">
<td>10</td>
<td>dolly</td>
<td>11697</td>
<td>0.067214</td>
</tr>
</tbody>
</table>
<h3 id="deita-data-efficient-instruction-tuning-for-alignment">Deita:
Data-Efficient Instruction Tuning for Alignment</h3>
<p>One sentence summary: The data were screened by scoring from three
dimensions: complexity, quality and diversity</p>
<h5 id="abstractconclusion">abstract&amp;conclusion</h5>
<img src="/2024/10/21/Implementation/11.png" srcset="/img/loading.gif" lazyload class="">
<img src="/2024/10/21/Implementation/12.png" srcset="/img/loading.gif" lazyload class="">
<h5 id="design-experiment-3">Design experiment</h5>
<p>Two kinds of data were collected, one was SOTA model data, with 300K
pieces, and the other was some ordinary base data, with 100K pieces.</p>
<ol type="1">
<li><p>Evol Complexity Given a small seed dataset <span
class="math display">\[D={(I_1^{(0)},R_1^{(0)}),... , (I_N ^ {} (0), R_N
^ {(0)})} \]</span>, for each command <span class="math display">\[I_k
\]</span>^ 0, increase the complexity of the instruction (increase
conditions, deepen instructions, specific instructions, improve
reasoning step). After M iterations, instructions of different
complexity can be obtained, <span class="math display">\[I_k^{(0)},...
,I_k^{(M)}\]</span>, and then use ChatGPT to sort and score these M
instructions.</p></li>
<li><p>Evol Quality The approach is basically the same, except that the
generated responses are used to let ChatGPT score the instruction
data.</p></li>
<li><p>Diversity Firstly, the data in the data pool is sorted in
descending order according to the comprehensive score of complexity and
quality (complexity score * quality score), and then sample data <span
class="math display">\[x_i\]</span>is extracted one by one in order to
calculate the distance value between <span
class="math display">\[x_i\]</span>and the nearest sample in the
screening pool. In this way, the data is represented by vector using the
Llama1-13B model. The distance is calculated using cosine similarity. If
the distance value is less than τ, it is considered that the sample is
not highly similar to the data in the screening pool and can be included
in the screening pool. Otherwise, it is not included in the screening
pool. When the number of samples in the screening pool reaches the
specified number of samples, the diversity screening is completed.
<img src="/2024/10/21/Implementation/13.png" srcset="/img/loading.gif" lazyload class=""></p></li>
</ol>
<p>Repetition reference site:
https://distilabel.argilla.io/1.0.2/sections/papers/deita/</p>
<h3 id="mods-model-oriented-data-selection-for-instruction-tuning">MoDS:
Model-oriented Data Selection for Instruction Tuning</h3>
<p>One sentence summary: A data filtering method based on the model's
own perception of instruction data</p>
<p>evaluate: It's a little too cumbersome, and the LLM needs to be
fine-tuned to be evaluated and is already a waste of time.</p>
<h5 id="design-experiment-4">Design experiment</h5>
<img src="/2024/10/21/Implementation/14.png" srcset="/img/loading.gif" lazyload class="">
<p>Step1: Utilize the reward-model-debertav3-large-v2 model which is
developed by OpenAssistant to score the triplet(Instruction, input,
output), obtains a score, when the score exceeds α, the data quality is
considered to meet the standard, and a high-quality dataset is
constructed.</p>
<p>Step2: The K-Center-Greedy algorithm is used to achieve diversity
screening <img src="/2024/10/21/Implementation/15.png" srcset="/img/loading.gif" lazyload class=""></p>
<p>Step3: The data set selected in the second step is used for initial
fine-tuning of the model. After obtaining the following ability of basic
instructions, the high-quality data set of the first step is used for
reasoning, and the result is scored by the reward model. When the score
is less than β, it indicates that the initial model cannot generate
high-quality responses to these instructions and does not have the
ability to process these types of instructions, so as to obtain the
necessary data set. Then, the necessity Data set row diversity is
filtered to obtain Augmented Instruction Data.</p>
<p>Step4: Merge these two datasets(Augmented datasets and seed
datasets), and then finetune the raw pre-trained LLM.</p>
<h5 id="reproduce-process-2">Reproduce process</h5>
<p>Step1: Use reward models to filter high-quality data <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python  ./quality-evaluation/quality-evaluation.py ./alpaca_data.json ./quality-evaluation.json<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./quality-evaluation/high-quality-data-extraction.py ./quality-evaluation.json ./high-quality-data.json 0.0<br></code></pre></td></tr></table></figure></p>
<p>The author made a small error that if run directly will report
TypeError: '&gt;' not supported between instances of 'float' and 'str',
line 103 of high-quality-data-extraction.py needs to be modified as
follows: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">float</span>(item[<span class="hljs-string">&#x27;reward_score&#x27;</span>]) &gt; <span class="hljs-built_in">float</span>(threshold):<br></code></pre></td></tr></table></figure></p>
<p>Step2: Filter diversity data top_k = 500 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./diverse-data-selection/run.py ./high-quality-data.json ./seed-instructions.json top_k<br></code></pre></td></tr></table></figure></p>
<p>Step3: Obtain enhanced data sets Remember to change the number of
Gpus before you start --nproc_per_node=GPU_numbers It took 6:31min on
the two A6000s <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./train/run.sh<br></code></pre></td></tr></table></figure></p>
<p>Inference: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./inference/instruction_filter.sh<br></code></pre></td></tr></table></figure></p>
<p>Necessity evaluation: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs script">python  ./necessity-evaluation/necessity-evaluation.py ./result.json ./inference-necessity-evaluation.json <br>python ./necessity-evaluation/necessity-instruction-extraction.py ./inference-necessity-evaluation.json ./<br>necessity-instruction.json 0<br></code></pre></td></tr></table></figure></p>
<p>all number of instructions 100</p>
<p>The percent of each score: (4, 5) : 1 0.01 (-5, -4) : 3 0.03 (5, 6) :
8 0.08 (-2, -1) : 16 0.16 (-3, -2) : 6 0.06 (2, 3) : 10 0.1 (7, 8) : 9
0.09 (6, 7) : 3 0.03 (-1, 0) : 9 0.09 (1, 2) : 10 0.1 (-7, -6) : 1 0.01
(0, 1) : 13 0.13 (8, 9) : 2 0.02 (3, 4) : 5 0.05 (-6, -5) : 2 0.02 (-4,
-3) : 2 0.02 num of bad case : 39</p>
<p>Merge data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">python ./necessity-evaluation/merge.py ./seed-instructions.json ./necessity-instruction.json  ./final-selected-dataset.json<br></code></pre></td></tr></table></figure> number of file 1 500</p>
<p>number of file 2 39</p>
<p>number of data 539</p>
<p>Step 4:Train LLM Fine-tuning the raw LLM with the merged dataset
again: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">bash ./train/run.sh<br></code></pre></td></tr></table></figure></p>
<h3
id="self-evolved-diverse-data-sampling-for-efficient-instruction-tuning">Self-Evolved
Diverse Data Sampling for Efficient Instruction Tuning</h3>
<p>One sentence summary: Use a self-iterative approach to get the most
differentiated training data</p>
<p>Evaluation: Is K-Center-Sampling too computation-intensive for large
training sets? And the evaluation structure also relies on GPT-4
judge.</p>
<h5 id="design-experiment-5">Design experiment</h5>
<img src="/2024/10/21/Implementation/16.png" srcset="/img/loading.gif" lazyload class="">
<p>There is the initial training pool <span
class="math inline">\(P_0\)</span>, and the training pool to be selected
is <span class="math inline">\(Q_0\)</span></p>
<ol type="1">
<li><p>In <span class="math inline">\(P_0\)</span> training model <span
class="math inline">\(M_0\)</span>, then embedding <span
class="math inline">\(Q_0\)</span> and <span
class="math inline">\(P_0\)</span></p></li>
<li><p>K-center-sampling algorithm is used to calculate the K points
farthest from <span class="math inline">\(P_0\)</span> in <span
class="math inline">\(Q_0\)</span> and select them as <span
class="math inline">\(S_0\)</span> and add them into <span
class="math inline">\(P_0\)</span> for the next round of training
data</p></li>
<li><p>Iterate</p></li>
</ol>
<h5 id="reproduce-process-3">Reproduce process</h5>
<p>First use the debug file to see if it works: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">CUDA_VISIBLE_DEVICES=0,1 nohup python -m torch.distributed.run --nproc_per_node=2 train.py --config_file configs/config_debug.yml --wandb_key 96a8969282485b1be0c7897993485cc613bf59aa &gt; train_log.out 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure></p>
<p>Once clear, start sifting through the data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs script">CUDA_VISIBLE_DEVICES=0,1 nohup python -m torch.distributed.run --nproc_per_node=2 train.py --config_file configs/config_example_kc_dolly.yml --wandb_key 96a8969282485b1be0c7897993485cc613bf59aa &gt; train_log.out 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure></p>
<p>For some reason, the author seems to use full fine-tuning, I have no
enough resources so can not finetuning...</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/PROJECT/" class="category-chain-item">PROJECT</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/PAPER/" class="print-no-link">#PAPER</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Implementation</div>
      <div>http://example.com/2024/10/21/Implementation/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Zhili Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>October 21, 2024</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/10/21/Project/" title="Project">
                        <span class="hidden-mobile">Project</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"7mJuq3fkcSAuEaGNgs89rWXu-gzGzoHsz","appKey":"WZ89ph5dYWgalvSmCqwuyaqg","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        Views: 
        <span id="leancloud-site-pv"></span>
        
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        Visitors: 
        <span id="leancloud-site-uv"></span>
        
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
